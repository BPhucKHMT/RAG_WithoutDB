0:00:01 - 0:00:07, Cuối cùng thì chúng ta sẽ cùng đến tìm
0:00:03 - 0:00:08, hiểu một số cái mô hình mà dữ liệu của
0:00:07 - 0:00:10, mình nó có cái mối quan hệ phi tuyến
0:00:08 - 0:00:14, tính thì chúng ta sẽ sử dụng những cái mô hình
0:00:10 - 0:00:16, nào chúng ta sẽ xét cái ví dụ sau đây
0:00:14 - 0:00:20, chúng ta có các cái điểm dữ liệu và cái
0:00:16 - 0:00:21, đường đi của cái hàm dự đoán của mình á
0:00:20 - 0:00:24, là nó sẽ không phải là một đường thẳng
0:00:21 - 0:00:26, mà nó sẽ là ở một cái dạng Đường Cong Và
0:00:24 - 0:00:28, ở đây chúng ta phỏng đoán chúng ta phỏng
0:00:26 - 0:00:30, đoán rằng là ở đây nó có hai cái điểm
0:00:28 - 0:00:32, cực tiểu một một điểm cực một điểm cực
0:00:30 - 0:00:36, đại thì chúng ta đoán Đây là một cái hàm
0:00:32 - 0:00:38, bậc ba đúng không Thì đối với cái dữ
0:00:36 - 0:00:41, liệu của mình mà có cái mối quan hệ phi
0:00:38 - 0:00:43, tuyến tính và chúng ta biết được cái
0:00:41 - 0:04:30, 43.3 Nhưng cái cách này thì nó đã bỏ qua
0:03:56 - 0:04:01, cái khoảng cách xa gần giữa cái điểm đặc trưng
0:04:00 - 0:04:04, với các
0:04:01 - 0:04:06, cái k cái láng giềng gần nhất này rồi do
0:04:04 - 0:04:08, đó cái cách số hai đó là chúng ta có thể
0:04:06 - 0:04:10, sử dụng một cái Công thức trung bình
0:04:08 - 0:04:13, trọng số theo khoảng cách nghĩa là
0:04:10 - 0:04:15, khoảng cách nào mà càng nhỏ thì trọng số
0:04:13 - 0:04:17, sẽ càng lớn và khoảng cách nào càng lớn
0:04:15 - 0:04:20, thì trọng số sẽ càng nhỏ như vậy là đây
0:04:17 - 0:04:23, là hai cách để tính trung bình và ước
0:04:20 - 0:04:26, đưa ra được cái ước lượng cái giá trị
0:04:23 - 0:04:30, output cho cái đặc trưng đầu vào là tại
0:04:26 - 0:04:36, đây thì cái mô hình K-Nearest Neighbor Đó là một
0:04:30 - 0:04:36, cái mô hình mà nó gọi là học lười (lazy
0:04:38 - 0:04:44, learning) Tại sao nó gọi là lazy learning
0:04:42 - 0:04:46, tại vì cái mô hình này nó không thật sự
0:04:44 - 0:04:50, mà nói nó không có học gì hết Nó không
0:04:46 - 0:04:52, có tạo ra những cái tham số của mô hình
0:04:50 - 0:04:54, ở đây nó chỉ có duy nhất một cái siêu
0:04:52 - 0:04:56, tham số là k láng giềng gần nhất này thôi
0:04:54 - 0:04:58, còn nó không có cái tham số của mô hình
0:04:56 - 0:05:02, và nó sẽ trông cậy hoàn toàn vô các cái
0:04:58 - 0:05:04, mẫu dữ liệu của mình do đó cái K-Nearest Neighbor
0:05:02 - 0:05:06, này nó sẽ rất là tốn tài nguyên
0:05:04 - 0:05:09, khi chúng ta triển khai Tại vì sao chúng
0:05:06 - 0:05:10, ta sẽ phải lưu trữ chúng ta sẽ phải lưu
0:05:09 - 0:05:13, lại hết tất cả các cái dữ liệu trong cái
0:05:10 - 0:05:16, dataset của mình để khi có một cái mẫu
0:05:13 - 0:05:19, dữ liệu mới chúng ta sẽ làm thao tác lookup
0:05:16 - 0:05:21, tra cứu cái điểm đặc trưng này so
0:05:19 - 0:05:23, với lại các cái điểm đặc trưng trong cái
0:05:21 - 0:05:26, tập dữ liệu huấn luyện xem cái đặc trưng
0:05:23 - 0:05:31, nào là gần nhất thì chúng ta sẽ lấy ra
0:05:26 - 0:05:33, và đó chính là cái điểm yếu của toán K-Nearest
0:05:31 - 0:05:35, Neighbor ý tưởng thì rất là đơn giản nhưng
0:05:33 - 0:05:38, mà khi chúng ta triển khai thì chúng ta
0:05:35 - 0:05:43, sẽ tốn rất nhiều bộ nhớ để mà lưu trữ
0:05:38 - 0:05:46, và để giải quyết cho cái dữ liệu mà
0:05:43 - 0:05:48, có cái dạng phức tạp phi tuyến tính thì
0:05:46 - 0:05:50, một trong những cái giải pháp cũng rất
0:05:48 - 0:05:53, là nổi tiếng đó chính là sử dụng mạng
0:05:50 - 0:05:58, Neural Network hay còn gọi là Multi-layer Perceptron
0:05:53 - 0:06:02, và trong thì trong Scikit-learn nó sẽ
0:05:58 - 0:06:04, có cái module tương ứng đó là MLP Và nếu
0:06:02 - 0:06:06, như chúng ta làm cho bài toán regression
0:06:04 - 0:06:06, thì nó sẽ là
0:06:06 - 0:06:12, MLP Regressor Còn nếu như chúng ta làm
0:06:10 - 0:06:16, cho bài toán phân loại thì đó là
0:06:12 - 0:06:19, MLP Classifier thì ý tưởng của Multi-layer Perceptron trong á
0:06:16 - 0:06:23, Tức là ngoài cái
0:06:19 - 0:06:28, input feature tức là toàn bộ
0:06:23 - 0:06:30, cái dữ kiện đầu vào ở đây và cái output
0:06:28 - 0:06:32, layer ở đây thì chúng ta ta sẽ có thêm
0:06:30 - 0:06:34, cái thành phần nó gọi là
0:06:32 - 0:06:38, hidden
0:06:34 - 0:06:43, layer và cái dữ liệu đầu ra của mình mà
0:06:38 - 0:06:45, nó càng phức tạp của cái đầu ra này của
0:06:43 - 0:06:48, mình mà càng phức tạp Tức là nó rất là
0:06:45 - 0:06:51, phi tuyến so với lại cái đầu vào thì
0:06:48 - 0:06:54, chúng ta sẽ phải tăng cái số hidden layer
0:06:51 - 0:06:56, này lên nó có thể tăng lên là hai lớp ba
0:06:54 - 0:06:59, lớp Và thậm chí trong một số bài toán đó
- 0:06:56 - 0:07:01, là nó có thể lên đến hàng trăm lớp và
- 0:06:59 - 0:07:04, đối với cái bài toán regression thì cái
- 0:07:01 - 0:07:07, đầu ra của mình do là cái miền giá trị
- 0:07:04 - 0:07:09, của mình Nó thuộc cái đoạn từ trừ vô
- 0:07:07 - 0:07:11, cùng cho đến cộng vô cùng do đó ở đây
- 0:07:09 - 0:07:13, chúng ta sẽ không có cái hàm kích hoạt ở
- 0:07:11 - 0:07:16, cuối Tại vì nếu như chúng ta sử dụng cái
- 0:07:13 - 0:07:17, hàm Sigmoid làm hàm kích hoạt thì nó sẽ ép cái
- 0:07:16 - 0:07:20, giá trị đầu ra xin lỗi Nó ép cái giá
- 0:07:17 - 0:07:24, trị output của mình về cái đoạn là từ 0
- 0:07:20 - 0:07:26, cho đến 1 đó thì cái điều này là sẽ giúp
- 0:07:24 - 0:07:28, cho sẽ làm cho cái mô hình của mình nó không
- 0:07:26 - 0:07:30, thể học được thì cái mạng Neural Network
- 0:07:28 - 0:07:33, này là một trong trong những cái mạng có
- 0:07:30 - 0:07:35, tính chất rất là tổng quát nó vừa có thể
- 0:07:33 - 0:07:37, dùng cho bài toán hồi quy nó vừa có thể
- 0:07:35 - 0:07:40, dùng cho bài toán phân
- 0:07:37 - 0:07:43, lớp và nó cũng rất là cơ động nó cho
- 0:07:40 - 0:07:46, phép mình có thể thêm số hidden layer số
- 0:07:43 - 0:07:48, lớp ẩn hoặc là giảm bớt để tùy vô cái dữ
- 0:07:46 - 0:07:50, liệu tính chất của dữ liệu của mình nếu
- 0:07:48 - 0:07:52, như dữ liệu của mình mà phức tạp thì
- 0:07:50 - 0:07:54, chúng ta sẽ tăng cái số layer lên còn
- 0:07:52 - 0:07:56, nếu mà dữ liệu của mình nó đơn giản hơn
- 0:07:54 - 0:07:57, thì chúng ta có thể giảm cái số layer
- 0:07:56 - 0:08:02, này
- 0:07:57 - 0:08:06, xuống và ngoài cái mô hình K-Nearest
- 0:08:02 - 0:08:09, Neighbor rồi Multi-layer Perceptron hoặc là
- 0:08:06 - 0:08:11, Neural Network thì chúng ta sẽ còn rất
- 0:08:09 - 0:08:14, nhiều những cái thuật toán khác để phục
- 0:08:11 - 0:08:17, vụ cho bài toán hồi quy đó ví dụ như là
- 0:08:14 - 0:08:20, chúng ta có Support Vector
- 0:08:17 - 0:08:24, Regressor cái phiên bản gốc của Support Vector
- 0:08:20 - 0:08:27, Machine (SVM)
- 0:08:24 - 0:08:31, sau này thì người ta có cái biến thể
- 0:08:27 - 0:08:34, là sử dụng cái mô hình Support Vector để cho cái
- 0:08:31 - 0:08:38, bài toán hồi quy. Decision Tree
- 0:08:34 - 0:08:42, Regressor, Random Forest Regressor và
- 0:08:38 - 0:08:44, Gradient Boosting (XGBoost, LightGBM, CatBoost) Regressor thì
- 0:08:42 - 0:08:46, đây là những cái mô hình rất là nổi
- 0:08:44 - 0:08:49, tiếng thường được sử dụng cho các cái
- 0:08:46 - 0:08:52, bài toán hồi quy và đạt được những cái
- 0:08:49 - 0:08:55, độ chính xác cao Đặc biệt là các cái
- 0:08:52 - 0:08:57, thuật toán nằm ở hai cái nhóm cuối này
- 0:08:55 - 0:09:00, và như vậy thì trong cái bài học ngày
- 0:08:57 - 0:09:03, hôm nay thì chúng ta đã tìm hiểu qua
- 0:09:00 - 0:09:07, khái niệm về mô hình hồi quy tuyến tính
- 0:09:03 - 0:09:10, mô hình hồi quy rồi mô hình phân lớp rồi
- 0:09:07 - 0:09:12, mô hình học có giám sát rồi chúng ta
- 0:09:10 - 0:09:16, cũng tìm hiểu qua các cái mô hình hồi quy
- 0:09:12 - 0:09:16, tuyến tính như là mô hình Linear
- 0:09:17 - 0:09:22, Regression rồi mô hình các cái biến thể của
- 0:09:21 - 0:09:24, nó ví dụ như là
- 0:09:22 - 0:09:29, Lasso
- 0:09:24 - 0:09:29, Regression, Ridge Regression và Elastic
- 0:09:30 - 0:09:36, Net thì đây là để giải quyết cho
- 0:09:33 - 0:09:36, các cái trường hợp
- 0:09:36 - 0:09:44, mà dữ liệu y của mình nó có mối quan hệ
- 0:09:40 - 0:09:46, tuyến tính cái mối quan hệ tuyến tính còn
- 0:09:44 - 0:09:49, trong trường hợp mà Non-Linear tức là phi
- 0:09:46 - 0:09:49, tuyến
- 0:09:50 - 0:09:54, tính thì chúng ta sẽ có các cái mô hình
- 0:09:53 - 0:09:56, ví dụ như
- 0:09:54 - 0:09:59, là bản chất là chúng ta vẫn có thể dùng
- 0:09:56 - 0:10:03, mô hình Linear Regression được nhưng mà chúng ta
- 0:09:59 - 0:10:03, phải kết hợp với cái bước gọi là Feature
- 0:10:05 - 0:10:10, Engineering chúng ta sẽ phải tạo thêm đặc
- 0:10:08 - 0:10:13, trưng rồi chúng ta sẽ có các cái mô hình
- 0:10:10 - 0:10:15, học lười lazy learning như là K-Nearest
- 0:10:13 - 0:10:19, Neighbor hoặc là một cái mô hình rất là
- 0:10:15 - 0:10:22, tổng quát đó là Multi-layer Perceptron
- 0:10:19 - 0:10:22, hay là mạng Neural
- 0:10:22 - 0:10:29, Network rồi chúng ta sẽ có các cái thuật
- 0:10:24 - 0:10:32, toán khác nữa SVR
- 0:10:29 - 0:10:36, là Support Vector Regressor rồi chúng ta sẽ có
- 0:10:32 - 0:10:39, Decision Tree Regressor rồi chúng ta sẽ có
- 0:10:36 - 0:10:43, Random Forest Regressor và các cái thuật
- 0:10:39 - 0:10:45, toán về Boosting ví dụ như là XGBoost, LightGBM
- 0:10:43 - 0:10:47, thì tất cả những cái thuật toán đó
- 0:10:45 - 0:10:50, nó đều có cái phiên bản cho bài toán hồi
- 0:10:47 - 0:10:50, quy