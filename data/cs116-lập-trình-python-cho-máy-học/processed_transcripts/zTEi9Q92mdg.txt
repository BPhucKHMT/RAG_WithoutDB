0:00:00 - 0:00:05, Cuối cùng thì chúng ta sẽ cùng tìm hiểu
0:00:02 - 0:00:06, một số cái mô hình à để giải quyết cho
0:00:05 - 0:00:10, cái trường hợp là dữ liệu có mối quan hệ
0:00:06 - 0:00:13, Phi tuyến thì mô hình đầu tiên đó chính
0:00:10 - 0:00:16, là mô hình caris neighbor classifier thì
0:00:13 - 0:00:18, cái ý tưởng của cái phương pháp này đó
0:00:16 - 0:00:20, chính là chúng ta sẽ tìm k cái náng diện
0:00:18 - 0:00:25, gần nhất trong trường hợp này K của mình
0:00:20 - 0:00:27, là bằng 3 k của mình bằng 3 và cái nhãn
0:00:25 - 0:00:29, của mình tương ứng với lại từng đặc
0:00:27 - 0:00:30, trưng nó sẽ là các cái màu Ví dụ như đây
0:00:29 - 0:00:32, là các điểm màu
0:00:30 - 0:00:36, màu hồng đây là các điểm màu nâu và đây
0:00:32 - 0:00:38, là các điểm màu đen thì với k = 3 và cái
0:00:36 - 0:00:41, điểm ở đây chính là cái điểm đặc trưng
0:00:38 - 0:00:44, đầu vào chúng ta sẽ tìm xem trong tất cả
0:00:41 - 0:00:46, những cái điểm mẫ dữ liệu ở đây thì k
0:00:44 - 0:00:49, cái láng diền gần nhất là k điểm nào và
0:00:46 - 0:00:51, đây chính là k cái điểm gần nhất và
0:00:49 - 0:00:52, chúng ta sẽ cùng thực hiện cái thao tác
0:00:51 - 0:00:56, gọi là
0:00:52 - 0:00:58, voting tức là chúng ta sẽ xem xem cái
0:00:56 - 0:01:00, nhãn nào được xuất hiện nhiều lần thì
0:00:58 - 0:01:03, trong ba cái điểm ở đây thì cái nhãn màu
0:01:00 - 0:01:04, đen nó xuất hiện nhiều nhất do đó thì
0:01:03 - 0:01:07, chúng ta sẽ kết luận là cái điểm đặc
0:01:04 - 0:01:09, trưng này sẽ nhận cái output của mình
0:01:07 - 0:01:14, chính là cái nhãn màu đen chúng ta sẽ
0:01:09 - 0:01:16, gán cái nhãn màu đen ở đây và đây cũng
0:01:14 - 0:01:20, là một trong những cái thuật toán nó gọi
0:01:16 - 0:01:22, là lazy learning lazy learning có nghĩa
0:01:20 - 0:01:26, là mô hình này nó sẽ không có tham số gì
0:01:22 - 0:01:28, hết và nó sẽ phải buộc để nhớ toàn bộ
0:01:26 - 0:01:30, tất cả những cái dữ liệu đầu vào khi
0:01:28 - 0:01:33, chúng ta tiến hành test chúng ta vẫn
0:01:30 - 0:01:34, phải lưu các cái tập dữ liệu Trend đó
0:01:33 - 0:01:36, thì điều này nó sẽ gây ra cái sự bất
0:01:34 - 0:01:39, tiện đó là nó tốn bộ
0:01:36 - 0:01:43, nhớ Nó sẽ tốn bộ
0:01:39 - 0:01:46, nhớ điểm mạnh của đó của nó đó chính là
0:01:43 - 0:01:48, nó có thể học được cho những cái tình
0:01:46 - 0:01:51, huống là phi tuến tính nó chỉ cần tìm k
0:01:48 - 0:01:54, cái láng diện gần nhất rồi sau đó nó sẽ
0:01:51 - 0:01:58, dựa trên cái nhãn của C cái láng gìn gần
0:01:54 - 0:01:59, nhất đó để mà nó đi gán cái nhãn tương
0:01:58 - 0:02:01, ứng cho cái đặc trưng đầu bào dựa trên
0:01:59 - 0:02:03, cái phương pháp là voting hoặc chúng ta
0:02:01 - 0:02:06, sẽ có một cái cải tiến của phương pháp
0:02:03 - 0:02:09, voting đó là VOT dựa trên cái trọng số
0:02:06 - 0:02:11, những cái cái điểm nào mà có cái khoảng
0:02:09 - 0:02:13, cách gần hơn thì nó sẽ cho cái trọng số
0:02:11 - 0:02:14, nó lớn hơn chúng ta tính toán cho nó cái
0:02:13 - 0:02:17, trọng số lớn
0:02:14 - 0:02:19, hơn và đây là một cái mô hình không có
0:02:17 - 0:02:22, tham số thì chúng ta đã đề cập như ở
0:02:19 - 0:02:24, trên mô hình này không có tham số cái k
0:02:22 - 0:02:25, Ở đây nó không được gọi là tham số mà nó
0:02:24 - 0:02:30, gọi là siê tham
0:02:25 - 0:02:32, số siê tham số của mô hình
0:02:30 - 0:02:35, còn cái tham số mà đưa ra cái quyết định
0:02:32 - 0:02:37, xem có bao nhiêu nó đưa ra cái quyết
0:02:35 - 0:02:41, định xem là với một cái đặc trưng đầu
0:02:37 - 0:02:43, vào thì chúng ta sẽ tính toán xử lý trên
0:02:41 - 0:02:45, cái tham số đó đ từ đó đưa ra qu cái
0:02:43 - 0:02:50, quyết định thì nó mới gọi là tham số của
0:02:45 - 0:02:53, mô hình tham số mô hình đó thì m cái cái
0:02:50 - 0:02:55, thuộc toán caris classifier này là nó
0:02:53 - 0:02:57, không có tham số cho mô hình nhưng mà nó
0:02:55 - 0:03:00, vẫ sẽ có một cái siêu tham số
0:02:57 - 0:03:02, k và mô hình tiếp tiếp theo đó là mô
0:03:00 - 0:03:05, hình multilayer percep hay còn gọi là
0:03:02 - 0:03:07, neural Network thì đây là một trong
0:03:05 - 0:03:09, những cái mô hình tổng quát nó giúp cho
0:03:07 - 0:03:11, chúng ta giải quyết được trong cái tình
0:03:09 - 0:03:13, huống là dữ liệu của mình có mối quan hệ
0:03:11 - 0:03:16, phụ thuộc một cách phi tuyến tính với
0:03:13 - 0:03:18, lại cái dữ liệu đầu vào bằng cách đó là
0:03:16 - 0:03:20, học ra những cái đặc trưng trung gian
0:03:18 - 0:03:22, giúp cho cái việc giải quyết à giúp cho
0:03:20 - 0:03:25, cái việc mà đưa ra cái quyết định nó dễ
0:03:22 - 0:03:26, dàng hơn thì ở đây chúng ta sẽ có một
0:03:25 - 0:03:29, lớp ẩn nhưng mà trong trường hợp tổng
0:03:26 - 0:03:32, quát thì cái số lượng h hidden layer này
0:03:29 - 0:03:34, chúng ta có thể tăng lên là nhiều lớp ẩn
0:03:32 - 0:03:37, có thể là 2 3 hoặc thậm chí là hàng trăm
0:03:34 - 0:03:40, lớp ẩn
0:03:37 - 0:03:44, đó đối với cái bài toán phân lớp mà
0:03:40 - 0:03:46, nhiều lớp thì output này nếu như K số
0:03:44 - 0:03:50, phân lớp của mình cần phân loại đó là
0:03:46 - 0:03:54, bằng Ờ Bằng bằng 3 đi thì chúng ta sẽ ra
0:03:50 - 0:03:56, ba cái neuron nếu như K mà bằng 5 thì nó
0:03:54 - 0:03:58, sẽ tạo ra là năm cái neuron như vậy thì
0:03:56 - 0:04:01, cái mạng neuro Network này của mình nó
0:03:58 - 0:04:03, có cái tính linh hoạt rất là cao nó vừa
0:04:01 - 0:04:06, có thể giúp cho chúng ta giải quyết được
0:04:03 - 0:04:08, các cái bài toán hồi quy và bài toán
0:04:06 - 0:04:10, phân lớ đối với bài toán hồi quy chúng
0:04:08 - 0:04:13, ta đã tìm hiểu trước đây thì cái output
0:04:10 - 0:04:14, của mình là nó sẽ ra một neuron và cái
0:04:13 - 0:04:17, neuron này thì chúng ta sẽ không có cái
0:04:14 - 0:04:19, hàm kích hoạt mà chúng ta chỉ có duy
0:04:17 - 0:04:22, nhất cái biến đổi tuyến tính từ cái lớp
0:04:19 - 0:04:24, phía trước đến cái lớp cuối cùng là một
0:04:22 - 0:04:26, nốt này thôi chúng ta sẽ không có hàm
0:04:24 - 0:04:28, kích hoạt để chúng ta ép cái mì giá trị
0:04:26 - 0:04:31, của mình về một cái khoảng nào
0:04:28 - 0:04:34, đó thì cái tính linh động của cái mạng
0:04:31 - 0:04:37, multilayer concept tron này rất là cao
0:04:34 - 0:04:40, và một trong những cái thuật toán một
0:04:37 - 0:04:43, trong những cái mô hình rất là hiệu quả
0:04:40 - 0:04:45, và mạnh mẽ và là tiền đề cho các cái
0:04:43 - 0:04:47, phương pháp ensemble về sau á đó chính
0:04:45 - 0:04:49, là mô hình decision tre thì đây là một
0:04:47 - 0:04:51, cái cấu trúc phân cấp và cũng không có
0:04:49 - 0:04:54, tham số cái mô hình này là một cái mô
0:04:51 - 0:04:57, hình không có tham số và mỗi một cái nút
0:04:54 - 0:05:00, mỗi một cái nút Ví dụ như đây là một cái
0:04:57 - 0:05:03, nút đây là một cái nút thì nó đại diện
0:05:00 - 0:05:05, cho một cái thuộc tính hay là một cái
0:05:03 - 0:05:07, đặc trưng nó đại diện cho một cái thuộc
0:05:05 - 0:05:09, tính hay đặc trưng chúng ta sẽ tiến hành
0:05:07 - 0:05:13, so sánh kiểm tra xem cái thuộc tính và
0:05:09 - 0:05:17, đặc trưng đó nó sẽ nhận cái giá trị
0:05:13 - 0:05:18, Ờ từ bao nhiêu đến bao nhiêu cái khoảng
0:05:17 - 0:05:21, giá trị của nó nó nằm trong cái đoạn nào
0:05:18 - 0:05:24, để chúng ta sẽ phân nhánh nó ra và ở đây
0:05:21 - 0:05:27, chúng ta sẽ có cái khái niệm là nhánh
0:05:24 - 0:05:29, nhấn là từ cái nút đó tương ứng với lại
0:05:27 - 0:05:31, một số trong số cái khả năng có xảy ra
0:05:29 - 0:05:35, của thuộc tính ví dụ chúng ta xét đến
0:05:31 - 0:05:38, cái yếu tố đó là trời có mưa hay không
0:05:35 - 0:05:44, thì chúng ta sẽ có hai cái nhánh có khả
0:05:38 - 0:05:47, năng xảy ra đó chính là có và
0:05:44 - 0:05:50, không Trời có mây hay không chứ Thì có
0:05:47 - 0:05:52, mây hay không thì có hoặc là không đối
0:05:50 - 0:05:54, với những cái giá trị mà dạng liên tục
0:05:52 - 0:05:57, Ví dụ như
0:05:54 - 0:05:59, lớp cái lớp học của mình đó thì chó
0:05:57 - 0:06:01, chúng ta có thể là đưa ra các cái nhánh
0:05:59 - 0:06:05, ví dụ Ví dụ như nếu nhán là giá trị là 1
0:06:01 - 0:06:07, là lớp 1 nhán là 2 nhấn là 3 như vậy thì
0:06:05 - 0:06:12, ở đây là chúng ta sẽ có thể là nhận được
0:06:07 - 0:06:16, 12 nhánh thì chánh đó là nó thể hiện là
0:06:12 - 0:06:18, số Cái khả năng mà có khả năng xảy ra
0:06:16 - 0:06:22, của cái thuộc tính nào
0:06:18 - 0:06:25, đó rồi mỗi một cái nút lá Tức là cái nút
0:06:22 - 0:06:28, cuối cùng Đây là những cái nốt lá
0:06:25 - 0:06:30, nè terminal nde Tức là những cái nốt lá
0:06:28 - 0:06:33, Đây là cái nốt nó sẽ sẽ đưa ra cái quyết
0:06:30 - 0:06:36, định phân loại cuối cùng dựa trên các
0:06:33 - 0:06:39, cái thuộc tính đã duyệt trước đó ví dụ
0:06:36 - 0:06:42, tại cái nút lá này thì để ra được cái
0:06:39 - 0:06:43, thuộc tính của mình ở đây thì để ra được
0:06:42 - 0:06:44, cái quyết định cuối cùng cái quyết định
0:06:43 - 0:06:47, phân loại Cuối Cùng Ở đây thì chúng ta
0:06:44 - 0:06:51, sẽ phải dựa vào các cái đặc trưng của
0:06:47 - 0:06:54, các cái nốt ở trước đó đó là nốt này Nốt
0:06:51 - 0:06:56, này và nốt này dựa trên ba cái nốt này
0:06:54 - 0:06:57, thì chúng ta sẽ đưa ra được cái quyết
0:06:56 - 0:07:00, định
0:06:57 - 0:07:02, là cuối cùng của là tại đây là phân lớp
0:07:00 - 0:07:05, nào thì đây chính là cái ý tưởng của
0:07:02 - 0:07:08, decision tree Cây quyết
0:07:05 - 0:07:11, định và ở đây chúng ta sẽ có một cái ví
0:07:08 - 0:07:15, dụ đó là nếu hôm nay trời nắng độ ẩm cao
0:07:11 - 0:07:17, và gió yếu thì hôm nay chúng ta có nên
0:07:15 - 0:07:21, đi chơi cầu lông hay không đó thì đây là
0:07:17 - 0:07:25, một cái cây quyết định trời nắng tức là
0:07:21 - 0:07:27, Sunny ha độ ẩm cao đúng không độ ẩm cao
0:07:25 - 0:07:31, thì chúng ta sẽ đến đây chúng ta sẽ xem
0:07:27 - 0:07:35, coi là độ ẩm của mình là cao tức là qua
0:07:31 - 0:07:37, đây và khi chúng ta đã có trời nắng độ
0:07:35 - 0:07:40, ấm cao thì ở đây chúng ta sẽ đưa ra
0:07:37 - 0:07:43, quyết định luôn đó là chúng ta sẽ không
0:07:40 - 0:07:46, có đi chơi chúng ta không đi và bất chấp
0:07:43 - 0:07:48, là gió có yếu hay không yếu thì chúng ta
0:07:46 - 0:07:51, cũng sẽ vẫn đưa ra cái quyết định là
0:07:48 - 0:07:56, không có đi chơi thì đây chính
0:07:51 - 0:07:59, là cái cái cái ý tưởng của tục toán
0:07:56 - 0:08:01, decision trong cái việc đó là ứng với
0:07:59 - 0:08:04, từng một cái nốt nè thì chúng ta sẽ có
0:08:01 - 0:08:06, một cái đặc trưng ở đây là chúng ta đang
0:08:04 - 0:08:08, nói về đặc trưng của thời tiết ở đây thì
0:08:06 - 0:08:10, chúng ta đang nói về đặc trưng của độ ẩm
0:08:08 - 0:08:13, và tương ứng với đặc trưng thời tiết
0:08:10 - 0:08:15, Chúng ta có ba nhánh thì ba nhánh này
0:08:13 - 0:08:18, tương ứng là ba cái khả năng xảy ra của
0:08:15 - 0:08:21, cái đặc trưng thời tiết đó là trời nắng
0:08:18 - 0:08:25, trời mây là trời
0:08:21 - 0:08:26, mưa rồi và bây giờ làm sao chúng ta có
0:08:25 - 0:08:29, thể xây dựng được một cái cây quyết định
0:08:26 - 0:08:31, từ một cái bộ dữ liệu huấn luyện bộ dữ
0:08:29 - 0:08:33, liệu huấn lị này thì chúng ta sẽ thấy là
0:08:31 - 0:08:36, năm cái cột đầu tiên á tương ứng nó sẽ
0:08:33 - 0:08:39, là feature và cái cột cuối cùng của mình
0:08:36 - 0:08:42, nó chính là cái giá trị output của mô
0:08:39 - 0:08:45, hình cần dự đoán đó là có đi chơi hay
0:08:42 - 0:08:47, không thì ở đây chúng ta sẽ dựa trên một
0:08:45 - 0:08:49, cái loại độ đo đó gọi là độ đo
0:08:47 - 0:08:52, information Gain information tức là
0:08:49 - 0:08:55, thông tin Gain Tức là cái sự gia tăng
0:08:52 - 0:08:57, thì nếu như chúng ta xây dựng cái cây
0:08:55 - 0:08:59, chúng ta cố gắng là chọn ra những cái
0:08:57 - 0:09:02, nhánh đi nào mà có cái information Gain
0:08:59 - 0:09:04, cao nhất cái mức độ mà thông tin của nó
0:09:02 - 0:09:06, nhiều nhất đó thì chúng ta sẽ dựa trên
0:09:04 - 0:09:09, độ đ information g để quyết định xem
0:09:06 - 0:09:13, chọn Đặc trưng nào để phân loại cho cái
0:09:09 - 0:09:15, nút tiếp theo thì ở đây muốn tính được
0:09:13 - 0:09:17, cái information Gain chúng ta sẽ phải có
0:09:15 - 0:09:20, thêm một cái khái niệm nữa đó là entropy
0:09:17 - 0:09:24, là thể hiện cái mức độ đa dạng cái mức
0:09:20 - 0:09:25, độ đa dạng của các cái loại trong cái
0:09:24 - 0:09:27, đối của các cái loại đối tượng trong cái
0:09:25 - 0:09:30, tập S của
0:09:27 - 0:09:32, mình và khi đó thì chúng ta có cái công
0:09:30 - 0:09:36, thức của entropy trong cái tập S là bằng
0:09:32 - 0:09:42, trừ của p p log p với y chạy từ 1 cho
0:09:36 - 0:09:45, đến C thì C này chính là cái số cái đối
0:09:42 - 0:09:47, tượng số cái đối tượng ở trong cái tập S
0:09:45 - 0:09:51, này của mình và information Gain nó sẽ
0:09:47 - 0:09:55, được tính bằng entropy của nde cha Tức
0:09:51 - 0:09:58, là cái nde ở phía trên trừ cho
0:09:55 - 0:10:01, entropy của các cái nốt ở phía
0:09:58 - 0:10:04, dưới expectation tức là trung bình của
0:10:01 - 0:10:07, cái entropy của các cái n phía dưới và
0:10:04 - 0:10:12, với s là tập hợp các cái đối tượng gồm
0:10:07 - 0:10:16, có c nhãn rồi p với y chạy từ 1 cho đến
0:10:12 - 0:10:18, C thì nó là tỷ lệ của cái nhãn thức y
0:10:16 - 0:10:21, trong cái tập S của mình thì ở đây chúng
0:10:18 - 0:10:25, ta sẽ lấy một cái ví dụ à giả sử như s
0:10:21 - 0:10:27, của mình nó sẽ bao gồm hai chúng ta sẽ
0:10:25 - 0:10:28, bao gồm được hai nhãn thôi là cộng và
0:10:27 - 0:10:30, trừ thôi trong đây chúng ta thấy là cộng
0:10:28 - 0:10:32, và trừ thôi
0:10:30 - 0:10:35, rồi thì ở đây chúng ta sẽ có một cái
0:10:32 - 0:10:37, tình huống đó là s sẽ bao gồm cộng trừ
0:10:35 - 0:10:42, cộng
0:10:37 - 0:10:47, trừ thì khi đó Ờ cái P của cộng nó sẽ là
0:10:42 - 0:10:48, bằng 1/2 đúng không và P của trừ nó sẽ
0:10:47 - 0:10:52, là bằng
0:10:48 - 0:10:54, 1/2 Và khi đó thì cái độ đo entropy của
0:10:52 - 0:11:00, mình
0:10:54 - 0:11:04, entropy của s nó sẽ là bằng trừ của 1/2
0:11:00 - 0:11:04, log 1/2
0:11:04 - 0:11:12, vậy sau đó lại trừ cho 1/2 log của 1/2
0:11:09 - 0:11:14, đó thì ở đây chúng ta sẽ thấy là cái
0:11:12 - 0:11:17, tính đa dạng của nó sẽ rất là cao khi
0:11:14 - 0:11:18, chúng ta Trừ lấy hai cái giá trị này
0:11:17 - 0:11:21, chúng ta cộng lại với nhau thì chúng ta
0:11:18 - 0:11:24, sẽ ra cái entropy của mình trong trường
0:11:21 - 0:11:27, hợp này nó chính là bằng
0:11:24 - 0:11:32, 1 rồi trong trong tình huống tiếp theo
0:11:27 - 0:11:35, đó là s của của mình mặc dù C của mình
0:11:32 - 0:11:36, là bằng 2 tức là có hai Nhãn là cộng và
0:11:35 - 0:11:41, trừ Nhưng nếu như
0:11:36 - 0:11:41, à chúng ta chỉ có bao gồm Toàn cái dấu
0:11:41 - 0:11:47, cộng và không có cái dấu trừ nào thì khi
0:11:44 - 0:11:50, đó P của cộng lúc này nó sẽ là bằng 1
0:11:47 - 0:11:52, 100 ph và P của trừ của mình trong
0:11:50 - 0:11:57, trường hợp này đó là bằng
0:11:52 - 0:12:00, 0 thì lúc này cái entropy entropy của
0:11:57 - 0:12:02, mình trong trường hợp này đ thì do cái
0:12:00 - 0:12:05, tính đa dạng của mình nó rất là thấp nó
0:12:02 - 0:12:07, chỉ có duy nhất một cái loại nào đó thôi
0:12:05 - 0:12:11, thì khi đó Cái entropy của mình trong
0:12:07 - 0:12:14, trường hợp này nó sẽ là bằng trừ của 1
0:12:11 - 0:12:17, log của 1 và trừ của 1 log 1 chính là
0:12:14 - 0:12:20, bằng bằng 0 rồi sau đó chúng ta sẽ trừ
0:12:17 - 0:12:22, cho 0 nhân cho log của 0 thì vì ở đây
0:12:20 - 0:12:25, chúng ta có cái số 0 ở đây nên xem như
0:12:22 - 0:12:29, là chúng ta sẽ không tính toán như vậy
0:12:25 - 0:12:31, entropy s nó thể hiện cái mức độ đa dạng
0:12:29 - 0:12:33, trong trường hợp s của mình chỉ đơn
0:12:31 - 0:12:36, thuần chứa một cái loại đối tượng thôi
0:12:33 - 0:12:39, thì độ đa dạng của mình là bằng 0 trong
0:12:36 - 0:12:41, trường hợp entropy của mình mà có cái sự
0:12:39 - 0:12:44, xuất hiện của cộng trừ đồng đều nhau Tức
0:12:41 - 0:12:47, là cái tính đa dạng giống nhau thì
0:12:44 - 0:12:51, entropy của mình nó sẽ tiến về về
0:12:47 - 0:12:54, 1 và khi đó thì à chúng ta luôn mong
0:12:51 - 0:12:57, muốn hướng đến là với một cái đặc trưng
0:12:54 - 0:12:59, nó phân loại nó sẽ tách biệt hai cái tập
0:12:57 - 0:13:01, cộng và trừ này ra làm hai phần
0:12:59 - 0:13:04, như vậy là chúng ta đang muốn hướng đến
0:13:01 - 0:13:06, cái tình huốn này đúng không Tức là khi
0:13:04 - 0:13:10, chúng ta phân lại ra thì cái nhánh ở bên
0:13:06 - 0:13:12, dưới nè nó chỉ bao gồm những cái tập
0:13:10 - 0:13:15, hoặc là dấu cộng hoặc là dấu trừ thôi
0:13:12 - 0:13:17, tức là cái entropy của mình nó là bằng 0
0:13:15 - 0:13:18, đó thì mình luôn mong muốn là cái
0:13:17 - 0:13:21, entropy ở dưới là
0:13:18 - 0:13:24, thấp đồng thời
0:13:21 - 0:13:25, Ờ chúng ta không mong muốn cái entropy
0:13:24 - 0:13:28, của mình nó cao tại vì nếu như chúng ta
0:13:25 - 0:13:30, phân loại xong mà các cái giá trị ở đây
0:13:28 - 0:13:32, cộng và tr trừ ở đây nó vẫn xuất hiện
0:13:30 - 0:13:34, song song với nhau tức là tỷ lệ giống
0:13:32 - 0:13:36, nhau thì như vậy là cái đặc trưng ở đây
0:13:34 - 0:13:39, nó sẽ không có cái tính phân biệt nó sẽ
0:13:36 - 0:13:41, không có cái tính phân biệt cao đó thì
0:13:39 - 0:13:43, để đại diện cho để đạt được cái mục tiêu
0:13:41 - 0:13:45, đó là cái entropy ở cái nốt lá này thấp
0:13:43 - 0:13:48, thì ở đây chúng ta sẽ phải có cái dấu
0:13:45 - 0:13:50, trừ ở đằng trước thì khi cái dấu trừ này
0:13:48 - 0:13:53, mà Ờ có cái dấu trừ này đằng trước mà
0:13:50 - 0:13:55, giá trị ở đây mà thấp à giá trị ở đây
0:13:53 - 0:13:58, thấp Tức là cái information Gain này là
0:13:55 - 0:14:01, cao mà information Gain cao tức là th
0:13:58 - 0:14:02, cái đặc trưng mà mình phân loại ở đây là
0:14:01 - 0:14:05, chúng ta sẽ làm những cái đặc trưng quan
0:14:02 - 0:14:06, trọng thì trên cái hình ở đây chúng ta
0:14:05 - 0:14:11, sẽ thấy là có hai
0:14:06 - 0:14:13, cái đặc trưng và với cái cách chọn đặc
0:14:11 - 0:14:16, trưng ở bên tay trái chúng ta thấy là nó
0:14:13 - 0:14:19, có cái information Gain cao là vì sao
0:14:16 - 0:14:21, cái tỷ trọng của những dấu cộng và những
0:14:19 - 0:14:23, cái dấu trừ Nó bất đối xứng nhau dấu
0:14:21 - 0:14:25, cộng nó lớn ác so với dấu trừ bên đây
0:14:23 - 0:14:27, thì dấu trừ nó lớn át so với dấu cộng
0:14:25 - 0:14:31, trong khi đó bên đây thì cái information
0:14:27 - 0:14:33, Gain của mình nó là thấp À nó thấp là do
0:14:31 - 0:14:35, các cái tỷ trọng của dấu cộng và dấu trừ
0:14:33 - 0:14:38, nó xem xem nó đồng đều nhau như vậy thì
0:14:35 - 0:14:40, mình không mong muốn à cái đặc trưng này
0:14:38 - 0:14:41, chúng ta không mong muốn đi theo cái con
0:14:40 - 0:14:44, đường này mà chúng ta luôn mong muốn
0:14:41 - 0:14:45, chia ra theo cái con đường này để cho
0:14:44 - 0:14:49, tạo ra những cái đặc trưng có cái tính
0:14:45 - 0:14:51, phân biệt đó phân phân biệt cao tức là
0:14:49 - 0:14:53, chúng ta sẽ phân hóa dấu cộng sẽ Dồn hết
0:14:51 - 0:14:55, về một bên và dấu trừ sẽ Dồn hết về một
0:14:53 - 0:14:57, bên thì đó chính là cái ý tưởng của DC
0:14:55 - 0:15:01, centry dựa trên cái độ đo information
0:14:57 - 0:15:03, Gain để mà chúng ta có thể chọn ra các
0:15:01 - 0:15:06, cái đặc trưng cho một cái nút trong cái
0:15:03 - 0:15:06, cây của mình