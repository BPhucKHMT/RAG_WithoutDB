0:00:01 - 0:00:13, [âm nhạc]
0:00:14 - 0:00:19, Hôm nay chúng ta sẽ đến với bài học thứ
0:00:15 - 0:00:22, tư đó là tiền xử lý dữ liệu thì về vị trí
0:00:19 - 0:00:25, của bài học này đó là nó nằm ở cái bước
0:00:22 - 0:00:28, gọi là data preprocessing. Đây là một cái
0:00:25 - 0:00:30, bước mà trước khi chúng ta tiến hành
0:00:28 - 0:00:32, huấn luyện cái mô hình. Đây có thể nói là
0:00:30 - 0:00:34, một trong những cái bước cực kỳ quan
0:00:32 - 0:00:38, trọng, tại vì chúng ta có cái câu đó là
0:00:34 - 0:00:39, Garbage in, garbage out. Tức là tất cả các cái
0:00:38 - 0:00:42, mô hình máy học của mình nếu như chúng
0:00:39 - 0:00:44, ta đưa những cái dữ liệu rác vào bên
0:00:42 - 0:00:46, trong cái mô hình thì cho dù cái mô hình
0:00:44 - 0:00:48, của mình có tốt như thế nào đi chăng nữa
0:00:46 - 0:00:50, thì cái hiệu suất của nó cũng sẽ bị giảm
0:00:48 - 0:00:54, nếu như cái dữ liệu của mình đưa vào
0:00:50 - 0:00:56, không được tiền xử lý cẩn thận. Và cái
0:00:54 - 0:00:59, nội dung chính của chúng ta sẽ bao gồm
0:00:56 - 0:01:01, năm phần sau đây. Ở hai phần đầu tiên thì
0:00:59 - 0:01:04, nó liên quan đến cái việc là chúng
0:01:01 - 0:01:06, ta làm sạch dữ liệu trên cái dữ liệu thô
0:01:04 - 0:01:09, đầu vào của mình sau khi đã thực hiện
0:01:06 - 0:01:12, xong cái bước là data validation. Thì
0:01:09 - 0:01:15, chúng ta sẽ phải phát hiện và xử lý
0:01:12 - 0:01:16, những cái dữ liệu mà bị thiếu. Rồi chúng
0:01:15 - 0:01:19, ta sẽ phát hiện và xử lý những cái
0:01:16 - 0:01:21, trường hợp dữ liệu bị ngoại lệ. Dữ liệu
0:01:19 - 0:01:24, ngoại lệ thì nó sẽ khác với lại dữ liệu
0:01:21 - 0:01:26, bị thiếu ở chỗ đó là các cái giá trị của
0:01:24 - 0:01:28, mình nó vẫn được điền đầy đủ. Tuy nhiên,
0:01:26 - 0:01:30, nếu xét trong cái phân bố của cái dữ
0:01:28 - 0:01:33, liệu của mình thì các cái giá trị
0:01:30 - 0:01:36, này nó có những cái đặc điểm là nó nằm
0:01:33 - 0:01:39, bên ngoài cái phạm vi tập trung, tập trung
0:01:36 - 0:01:41, dày đặc của dữ liệu. Tức là nó
0:01:39 - 0:01:44, nằm ngoài cái phạm vi của dữ liệu của
0:01:41 - 0:01:46, mình rất là nhiều. Đó thì chúng ta sẽ tìm
0:01:44 - 0:01:50, cách phát hiện và xử lý cho những cái
0:01:46 - 0:01:52, loại giá trị như vậy. Và ba bước cuối thì
0:01:50 - 0:01:55, nó liên quan đến một cái từ khóa đó là
0:01:52 - 0:01:58, đặc trưng: tạo đặc trưng, biến đổi đặc
0:01:55 - 0:02:01, trưng và lựa chọn đặc trưng. Thì ở đây
0:01:58 - 0:02:03, cái khái niệm đặc trưng nó cũng chính là
0:02:01 - 0:02:07, dữ liệu của mình, tuy nhiên nó ở cái cấp
0:02:03 - 0:02:10, độ cao hơn và nó là những cái mà chúng
0:02:07 - 0:02:13, ta chuẩn bị để đưa vào cho mô hình máy
0:02:10 - 0:02:15, học. Thì ba cái bước cuối cùng chính là ba bước
0:02:13 - 0:02:16, chúng ta thao tác trên đặc trưng để
0:02:15 - 0:02:18, chuẩn bị đưa vào cho một cái mô hình máy
0:02:16 - 0:02:21, học. Đầu tiên thì chúng ta sẽ đến với
0:02:18 - 0:02:24, phần xác định và xử lý những cái dữ liệu
0:02:21 - 0:02:25, bị
0:02:24 - 0:02:28, thiếu.
0:02:25 - 0:02:31, Nhắc lại đó là để phát hiện những
0:02:28 - 0:02:34, cái dữ liệu bị thiếu ở trong phần EDA, tức
0:02:31 - 0:02:36, là cái phần mà phân tích dữ liệu, thì
0:02:34 - 0:02:39, chúng ta biết rằng là trong pandas, trong
0:02:36 - 0:02:40, cái thư viện pandas thì nó đã có cung
0:02:39 - 0:02:43, cấp các cái hàm để giúp cho mình phát
0:02:40 - 0:02:46, hiện cái dữ liệu bị thiếu đó là hàm isnull
0:02:43 - 0:02:48, hoặc là hàm isna. Thì chúng ta
0:02:46 - 0:02:51, có thể sử dụng cả hai hàm này để kiểm
0:02:48 - 0:02:53, tra xem bảng hoặc là cột hoặc là cái
0:02:51 - 0:02:56, dòng dữ liệu của mình có bị thiếu dữ
0:02:53 - 0:02:59, liệu hay không. Và ở bên trái đó là chúng
0:02:56 - 0:03:01, ta sẽ có một cái mẫu dữ liệu ha. Thì ở
0:02:59 - 0:03:04, đây chúng ta sẽ thấy là các cái dữ liệu
0:03:01 - 0:03:07, NaN nó nằm ở đây và khi chúng ta kiểm tra
0:03:04 - 0:03:10, thì nó sẽ ra là hai cái giá trị true ở
0:03:07 - 0:03:14, đây, tức là chúng ta sẽ có những cái vị
0:03:10 - 0:03:18, trí mà giá trị của mình nó bằng NaN, tức là
0:03:14 - 0:03:21, không phải là một cái giá trị xác
0:03:18 - 0:03:24, định. Thì sau khi chúng ta đã phát
0:03:21 - 0:03:26, hiện xong thì chúng ta sẽ tiến hành xử
0:03:24 - 0:03:28, lý các cái dữ liệu bị thiếu này. Thì
0:03:26 - 0:03:31, chúng ta sẽ có ba cái cách tiếp cận
0:03:28 - 0:03:33, chính. Cách tiếp cận đầu tiên đó chính
0:03:31 - 0:03:36, là chúng ta sẽ loại
0:03:33 - 0:03:39, bỏ. Đây là cách tiếp cận đơn giản nhất,
0:03:36 - 0:03:42, dễ thực hiện nhất. Cứ khi chỗ nào ở trên
0:03:39 - 0:03:46, hàng hoặc là trên cột mà chúng ta thấy
0:03:42 - 0:03:50, là có một cái tỉ lệ dữ liệu bị thiếu mà
0:03:46 - 0:03:53, đủ lớn, ví dụ như là trên 50% dữ liệu
0:03:50 - 0:03:57, của một cột hoặc là trên 50% dữ liệu
0:03:53 - 0:04:00, của một hàng mà có cái giá trị là NaN thì
0:03:57 - 0:04:02, chúng ta sẽ loại bỏ nó đi. Thì để thực
0:04:00 - 0:04:05, hiện cái việc loại bỏ này, chúng
0:04:02 - 0:04:08, ta có thể sử dụng cái hàm là dropna
0:04:05 - 0:04:11, hoặc là chúng ta có thể sử dụng cái
0:04:08 - 0:04:14, hàm drop column nếu như chúng ta đặt ra
0:04:11 - 0:04:18, những cái tiêu chí là bao nhiêu phần
0:04:14 - 0:04:20, trăm dữ liệu trở lên mà bị gọi là bị
0:04:18 - 0:04:23, rỗng, bị thiếu thì chúng ta mới loại bỏ
0:04:20 - 0:04:25, đi. Đó và hướng tiếp cận thứ hai đó là
0:04:23 - 0:04:27, hướng tiếp cận thay thế, có thể là thay
0:04:25 - 0:04:30, thế đơn biến, đa biến hoặc là thay thế
0:04:27 - 0:04:33, trên cái dữ liệu theo chuỗi thời gian.
0:04:30 - 0:04:35, Và trong thư viện scikit-learn thì chúng ta
0:04:33 - 0:04:40, cũng sẽ có cái API để phục vụ cho cái
0:04:35 - 0:04:42, việc thay thế này đó chính là scikit-learn
0:04:40 - 0:04:45, imputation thì chúng ta có thể tham khảo
0:04:42 - 0:04:47, trong cái đường link ở sau đây. Và
0:04:45 - 0:04:49, cái phương pháp nâng cao hơn đó chính là
0:04:47 - 0:04:52, chúng ta sẽ sử dụng chính những cái mô
0:04:49 - 0:04:56, hình máy học để đưa ra cái dự đoán cái
0:04:52 - 0:05:00, giá trị bị thiếu này. Thì sơ đồ dưới
0:04:56 - 0:05:03, đây đó là chúng ta sẽ có những cái
0:05:00 - 0:05:07, tiếp cận được chia ra thành các cái
0:05:03 - 0:05:09, nhánh. Đầu tiên đó là cái nhánh mà loại
0:05:07 - 0:05:12, bỏ. Và đây thì chúng ta có các cái chiến
0:05:09 - 0:05:14, thuật loại bỏ. Ví dụ như chúng ta sẽ loại
0:05:12 - 0:05:16, bỏ những cái giá trị nào mà bị thiếu
0:05:14 - 0:05:19, thôi, hoặc chúng ta loại bỏ theo
0:05:16 - 0:05:21, hàng hoặc chúng ta sẽ loại bỏ theo cột.
0:05:19 - 0:05:23, Đối với cái hướng tiếp cận thay thế thì
0:05:21 - 0:05:26, chúng ta sẽ có hai hướng đó là cơ bản và
0:05:23 - 0:05:29, nâng cao. Cơ bản thì chúng ta có thể thay
0:05:26 - 0:05:31, thế, ví dụ như thay bằng một cái giá trị
0:05:29 - 0:05:33, hằng số, hoặc là chúng ta có thể thay
0:05:31 - 0:05:35, những cái giá trị mang tính chất thống
0:05:33 - 0:05:38, kê, ví dụ như là giá trị min, giá trị
0:05:35 - 0:05:42, median, giá trị Mode hoặc là cái giá trị
0:05:38 - 0:05:44, mà có cái số lần xuất hiện là nhiều nhất.
0:05:42 - 0:05:47, Và còn một cái cách tiếp cận tự động nữa
0:05:44 - 0:05:49, đối với dữ liệu mà dạng time series, tức
0:05:47 - 0:05:51, là dữ liệu theo chuỗi thời gian, chúng ta
0:05:49 - 0:05:53, có thể căn cứ dựa trên cái chuỗi thời
0:05:51 - 0:05:57, gian để chúng ta có thể nội suy ra cái
0:05:53 - 0:06:00, giá trị mà mình sẽ điền khuyết vào bên
0:05:57 - 0:06:01, trong. Và ở hướng tiếp cận nâng cao thì đó
0:06:00 - 0:06:04, chính là các cái hướng tiếp cận
0:06:01 - 0:06:06, dựa trên mô hình máy học như là K-nearest
0:06:04 - 0:06:09, neighbor hoặc là các cái mô hình máy học
0:06:06 - 0:06:11, khác. Và ngoài ra thì chúng ta sẽ còn một
0:06:09 - 0:06:13, cái hướng tiếp cận khác không được nói
0:06:11 - 0:06:16, chi tiết ở đây đó là chúng ta sẽ tạo ra
0:06:13 - 0:06:18, thêm một cái cột mới. Trong cái cột mới
0:06:16 - 0:06:21, này thì chúng ta sẽ đánh dấu
0:06:18 - 0:06:23, là những cái dòng dữ liệu nào thì bị
0:06:21 - 0:06:25, thiếu và những cái dòng dữ liệu nào là
0:06:23 - 0:06:28, không bị thiếu. Thì đây là những cái
0:06:25 - 0:06:30, hướng tiếp cận chính cho cái việc xử lý
0:06:28 - 0:06:35, dữ liệu bị thiếu của mình.
0:06:30 - 0:06:38, Ờ trong cái bảng sau thì nó tiến hành đó
0:06:35 - 0:06:40, là so sánh các cái phương pháp, cách thức
0:06:38 - 0:06:43, thực hiện và đặc điểm của từng cái
0:06:40 - 0:06:44, phương pháp. Phương pháp đầu tiên đó là
0:06:43 - 0:06:47, chúng ta thay thế các cái đặc trưng đơn
0:06:44 - 0:06:49, biến. Thì chúng ta có thể thay thế các
0:06:47 - 0:06:52, cái giá trị mang tính chất thống kê như
0:06:49 - 0:06:56, là giá trị trung bình, giá trị trung vị
0:06:52 - 0:06:58, và giá trị Mode. Và cách thực hiện đó là
0:06:56 - 0:07:00, cũng rất là đơn giản đó là thay các cái
0:06:58 - 0:07:03, giá trị còn thiếu bằng những cái giá trị
0:07:00 - 0:07:04, trung bình, trung vị hoặc là những
0:07:03 - 0:07:07, cái giá trị mà xuất hiện thường xuyên
0:07:04 - 0:07:09, nhất của một cái biến nào đó, được thực
0:07:07 - 0:07:13, hiện một cách độc lập. Tức là chúng ta sẽ
0:07:09 - 0:07:15, xét trên một cái biến X_i. Ví dụ ở đây
0:07:13 - 0:07:22, chúng ta có rất nhiều những cái đặc
0:07:15 - 0:07:24, trưng là X1, X2, vân vân, cho đến Xn.
0:07:22 - 0:07:26, Thì chúng ta xét một cái X_i nào đấy, tức
0:07:24 - 0:07:28, là xét một cái cột nào đó và chúng ta sẽ
0:07:26 - 0:07:31, tính cái giá trị trung bình cho những
0:07:28 - 0:07:34, cái phần tử mà có giá trị. Hoặc là chúng
0:07:31 - 0:07:36, ta sẽ tính cái giá trị trung vị. Hoặc là
0:07:34 - 0:07:38, chúng ta sẽ đếm những cái giá trị nào có
0:07:36 - 0:07:39, tần suất xuất hiện nhiều nhất. Thì chúng
0:07:38 - 0:07:44, ta sẽ lấy những cái giá trị đại diện đó
0:07:39 - 0:07:47, điền vào những cái vị trí mà có bị rỗng,
0:07:44 - 0:07:50, bị thiếu giá trị. Thì cái phương pháp này
0:07:47 - 0:07:53, nó thứ nhất đó là nó đơn giản, dễ hiểu và
0:07:50 - 0:07:55, dễ thực hiện. Tuy nhiên cái giá trị đại
0:07:53 - 0:07:58, diện này thực sự mà nói nó cũng khó phản
0:07:55 - 0:08:00, ánh đúng được cái giá trị bị thiếu. Rõ
0:07:58 - 0:08:02, ràng là chúng ta không thể nào, à với một
0:08:00 - 0:08:03, cái vị trí bị thiếu chúng ta có thể điền
0:08:02 - 0:08:06, vào duy nhất một cái giá trị đại diện
0:08:03 - 0:08:09, như vậy được. Thì cũng tương tự như vậy,
0:08:06 - 0:08:11, cũng tương tự như cái phương pháp mà
0:08:09 - 0:08:13, thay thế các cái đặc trưng đơn biến, thì
0:08:11 - 0:08:16, chúng ta thay thế bằng một cái hằng số.
0:08:13 - 0:08:19, Hằng số này có thể là một con số rất là
0:08:16 - 0:08:21, lớn, ví dụ như là trừ 999 hoặc là con số
0:08:19 - 0:08:23, à xin lỗi, những cái giá trị rất là nhỏ
0:08:21 - 0:08:28, hoặc là giá trị rất là lớn, ví dụ như
0:08:23 - 0:08:30, -999, +9999 hoặc là con số trung dung,
0:08:28 - 0:08:34, ví dụ như là số 0.
0:08:30 - 0:08:37, Hoặc là cái con số mà vừa đại diện cho
0:08:34 - 0:08:40, một cái giá trị mà mang tính chất đại
0:08:37 - 0:08:43, diện ngoại lệ. Ví dụ như các cái biến
0:08:40 - 0:08:45, của mình ở bên trong cái cột dữ liệu X_i
0:08:43 - 0:08:48, đó là những con số dương thì chúng ta có
0:08:45 - 0:08:50, thể điền đó là một con số là -1 để hàm ý
0:08:48 - 0:08:52, rằng là à cái giá trị này nó là một giá
0:08:50 - 0:08:54, trị ngoại lệ nhưng mà nó vẫn có cái dạng
0:08:52 - 0:08:57, là số học để chúng ta có thể xử lý tính
0:08:54 - 0:08:59, toán được. Thì tương tự như cái phương
0:08:57 - 0:09:02, pháp thay thế đặc trưng đơn biến thì cái
0:08:59 - 0:09:05, phương pháp này nó đơn giản và nó cũng
0:09:02 - 0:09:08, khó phản ánh đúng được cái giá trị thực
0:09:05 - 0:09:10, sự mà bị thiếu ở đó là gì. Và những cái
0:09:08 - 0:09:14, tiếp cận sau thì có vẻ là nó sẽ logic
0:09:10 - 0:09:16, hơn, nó có cái căn cứ tốt hơn để giúp cho
0:09:14 - 0:09:19, chúng ta thay thế cái giá trị bị thiếu. Tiếp
0:09:16 - 0:09:21, cận đầu tiên đó là chúng ta sử dụng
0:09:19 - 0:09:24, phương pháp K-nearest
0:09:21 - 0:09:27, neighbor. Ví dụ như à ở đây chúng ta có
0:09:24 - 0:09:30, cái đặc trưng X_i là đặc trưng bị thiếu,
0:09:27 - 0:09:34, thì chúng ta sẽ xây dựng một cái mô hình
0:09:30 - 0:09:36, tìm kiếm k láng giềng gần nhất và
0:09:34 - 0:09:40, nó sẽ dựa trên các cái đặc trưng đó là
0:09:36 - 0:09:44, X1, X2 cho đến X_i-1 và X_i+1 cho đến
0:09:40 - 0:09:46, Xn, tức là loại bỏ đi cái X_i. Chúng ta sẽ
0:09:44 - 0:09:49, dùng các cái đặc trưng còn lại như là
0:09:46 - 0:09:53, cái đặc trưng của mô hình máy học và chúng
0:09:49 - 0:09:57, ta sẽ xem xét xem là ứng với cái dòng dữ
0:09:53 - 0:10:00, liệu đó thì nó sẽ có những cái giá trị
0:10:00 - 0:10:05, thì chúng ta sẽ điền cái giá trị còn
0:10:03 - 0:10:09, thiếu vào. Thì bản chất của cái phương
0:10:05 - 0:10:11, pháp K-nearest neighbor ở đây đó là
0:10:09 - 0:10:15, chúng ta sẽ thay thế các cái giá trị bị
0:10:11 - 0:10:19, thiếu bằng cái giá trị trung bình hoặc
0:10:15 - 0:10:20, là tổng trọng số của k láng giềng
0:10:19 - 0:10:22, trong cái không gian đặc trưng. Và cái
0:10:20 - 0:10:25, không gian đặc trưng ở đây thì được hiểu
0:10:22 - 0:10:29, đó là tất cả các cái đặc trưng mà trừ
0:10:25 - 0:10:30, cái đặc trưng mà đang bị thiếu ở đây là
0:10:29 - 0:10:33, X_i.
0:10:30 - 0:10:36, Thì phương pháp này nó sẽ chính xác hơn
0:10:33 - 0:10:39, và nó có cái tính giải thích được. Tuy
0:10:36 - 0:10:42, nhiên nó cũng sẽ gọi là có thể tốn kém
0:10:39 - 0:10:45, hơn về mặt chi phí tính toán do nó cũng
0:10:42 - 0:10:47, phải thực hiện cái phép so sánh K láng
0:10:45 - 0:10:50, giềng để tìm ra K láng giềng gần nhất trên một
0:10:47 - 0:10:52, cái tập dữ liệu rất là lớn. Và tương tự
0:10:50 - 0:10:54, như phương pháp K-nearest neighbor thì
0:10:52 - 0:10:57, chúng ta sẽ có cái phương pháp gọi là
0:10:54 - 0:10:59, nội suy tuyến tính. Chúng ta sẽ thay thế
0:10:57 - 0:11:03, cái giá trị bị thiếu bằng cái giá
0:11:03 - 0:11:10, trị được nội suy một cách tuyến
0:11:07 - 0:11:11, tính dựa trên các cái điểm dữ liệu mà
0:11:10 - 0:11:16, không bị thiếu lân cận. Tức là chúng ta
0:11:11 - 0:11:18, sẽ thực hiện gọi là cộng trung bình và
0:11:16 - 0:11:21, chúng ta sẽ nội suy ra cái giá trị bị
0:11:18 - 0:11:23, thiếu ở đó. Đó thì phương pháp này nó
0:11:21 - 0:11:26, cũng tương tự như cái phương pháp K-nearest
0:11:23 - 0:11:29, neighbor ở đây. Và đặc điểm của nó đó là nó sẽ
0:11:26 - 0:11:32, sử dụng cái mối quan hệ tuyến tính giữa
0:11:29 - 0:11:36, các cái điểm dữ liệu. Nó sử dụng ví dụ
0:11:32 - 0:11:38, cái giá trị của mình là tại đây là với
0:11:36 - 0:11:42, cái đặc trưng X thì chúng ta sẽ có cái
0:11:38 - 0:11:45, giá trị tương ứng của nó là A. Thì tại
0:11:42 - 0:11:48, cái vị trí tiếp theo là Y thì chúng ta
0:11:45 - 0:11:51, sẽ có giá trị là B. Và chúng ta sẽ thực
0:11:48 - 0:11:52, hiện cái giá trị bị thiếu ở đây là Z. Thì
0:11:51 - 0:11:56, chúng ta sẽ đoán xem cái giá trị ở đây
0:11:52 - 0:11:59, là gì. Đó thì chúng ta sẽ dựa trên X, Y, A
0:11:56 - 0:12:02, và Z để chúng ta nội suy ra cái giá trị
0:12:02 - 0:12:08, chấm hỏi ở giữa đây. Đó là cái idea của
0:12:05 - 0:12:10, phương pháp là nội suy tuyến tính. Và
0:12:08 - 0:12:12, phương pháp này thì nó có thể phù hợp
0:12:10 - 0:12:14, với mọi cái loại dữ liệu, tức là dữ liệu
0:12:12 - 0:12:18, của mình cho dù là loại dữ liệu gì thì
0:12:14 - 0:12:21, chúng ta cũng có thể thực hiện
0:12:18 - 0:12:23, được. Rồi nâng cấp hơn so với lại hai
0:12:21 - 0:12:26, phương pháp này đó là chúng ta sẽ thay
0:12:23 - 0:12:27, thế bằng phương pháp hồi
0:12:26 - 0:12:30, quy. Thì với cái phương pháp hồi quy
0:12:27 - 0:12:34, này thì bản chất nó cũng là một cái mô
0:12:30 - 0:12:37, hình máy học. Trong đó cái thành phần bị
0:12:34 - 0:12:40, thiếu X_i nó sẽ được tính toán dựa trên
0:12:37 - 0:12:41, các cái thành phần đặc trưng còn lại. Và
0:12:40 - 0:12:43, chúng ta giả sử rằng là X_i này nó sẽ phụ
0:12:41 - 0:12:45, thuộc một cách tuyến tính so với lại các
0:12:43 - 0:12:47, cái đặc trưng còn lại. Thì từ đó mình sẽ
0:12:45 - 0:12:50, xây dựng một cái mô hình máy học để dự
0:12:47 - 0:12:51, đoán cái giá trị X_i từ các cái giá trị
0:12:50 - 0:12:54, còn lại này. Thì phương pháp này nó sẽ
0:12:51 - 0:12:56, cho cái kết quả là chính xác hơn so với cái
0:12:54 - 0:12:59, việc là điền các cái giá trị cố định ở
0:12:56 - 0:13:02, đây ha. Và tuy nhiên nó có thể gây ra cái
0:12:59 - 0:13:05, hiện tượng đa cộng tuyến tức là hoặc
0:13:02 - 0:13:07, là quá khớp nếu như cái đặc trưng có mối
0:13:05 - 0:13:10, liên quan cao với các cái đặc trưng
0:13:07 - 0:13:13, khác. Nghĩa là nếu như chúng ta giả sử
0:13:10 - 0:13:15, cái yếu tố là đặc trưng X_i nó phụ thuộc
0:13:13 - 0:13:17, một cách tuyến tính so với lại các cái
0:13:15 - 0:13:20, đặc trưng còn lại thì nó sẽ gây ra cái
0:13:17 - 0:13:23, hiện tượng đa cộng tuyến đó. Và trong
0:13:20 - 0:13:25, thực tế thì rõ ràng không phải cái X_i,
0:13:23 - 0:13:28, cái đặc trưng mà đang bị thiếu dữ liệu ở
0:13:25 - 0:13:31, đây ha, nó có cái mối quan hệ tuyến tính.
0:13:28 - 0:13:32, Rõ ràng không phải lúc nào cũng như vậy.
0:13:31 - 0:13:34, Do đó thì để thực hiện được cái phương
0:13:32 - 0:13:37, pháp này chúng ta phải có một cái giả
0:13:34 - 0:13:39, định, phải có một cái giả định. Và cái giả
0:13:37 - 0:13:44, định này thì không phải lúc nào cũng có
0:13:39 - 0:13:48, thể thỏa mãn. Và cuối cùng đó là chúng
0:13:44 - 0:13:50, ta có thể thay thế dựa trên các cái mô
0:13:48 - 0:13:53, hình. Và mô hình này á là những cái mô
0:13:50 - 0:13:55, hình mà phi
0:13:53 - 0:13:59, tuyến. Thì có thể thay thế bằng các cái
0:13:55 - 0:14:01, mô hình dự đoán nâng cao đó và sử dụng
0:13:59 - 0:14:03, các cái mô hình máy học để ước tính các
0:14:01 - 0:14:06, giá trị còn thiếu
0:14:03 - 0:14:08, dựa trên các cái dữ liệu được quan sát.
0:14:06 - 0:14:12, Thì phương pháp này là phương pháp cao
0:14:08 - 0:14:14, nhất và tổng quát nhất tại vì các cái mô
0:14:12 - 0:14:16, hình máy học ở đây thì nó sẽ không có
0:14:14 - 0:14:18, một cái giả định là tuyến tính mà ở đây
0:14:16 - 0:14:21, chúng ta giả định nó là một cái mối quan
0:14:18 - 0:14:23, hệ phi tuyến luôn. Thì như vậy thì phương
0:14:21 - 0:14:26, pháp này nó sẽ cho cái độ chính xác cao
0:14:23 - 0:14:29, hơn. Tuy nhiên nó sẽ gặp cái vấn đề đó là
0:14:26 - 0:14:30, nó có thể phức tạp và tốn kém cho cái
0:14:29 - 0:14:33, quá trình tính toán, tại vì các cái mô
0:14:30 - 0:14:35, hình mà phi tuyến á thì thông thường đó
0:14:33 - 0:14:37, là cái chi phí tính toán của nó lớn do
0:14:35 - 0:14:40, nó phải thực hiện cái số phép biến đổi
0:14:37 - 0:14:40, nhiều hơn.