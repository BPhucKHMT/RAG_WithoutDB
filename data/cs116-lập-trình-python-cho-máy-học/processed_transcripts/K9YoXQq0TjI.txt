0:00:00 - 0:00:06, cuối cùng đó là kỹ thuật boosting thì
0:00:03 - 0:00:10, đây là một trong những cái kỹ thuật rất
0:00:06 - 0:00:13, là quan trọng của ensemble model à boosting
0:00:10 - 0:00:17, nó sẽ huấn luyện một cách tuần tự nếu
0:00:13 - 0:00:20, như cái thuật toán bagging á là các cái Model
0:00:17 - 0:00:22, được thực hiện một cách là song song và
0:00:20 - 0:00:27, độc lập với nhau thì boosting nó sẽ thực
0:00:22 - 0:00:30, hiện một cách tuần tự và mô hình sau mô
0:00:27 - 0:00:33, hình sau sẽ được train dựa theo cái kết
0:00:30 - 0:00:37, quả của cái mô hình trước và nhiệm vụ
0:00:33 - 0:00:39, của nó đó là cố gắng đi sửa sai đi đi
0:00:37 - 0:00:42, sửa sai nhưng mà cái sai này là những
0:00:39 - 0:00:45, cái sai còn lại thôi cái sai của các
0:00:42 - 0:04:23, cái mô hình trước đó nó đã nó đã đã hình
0:04:23 - 0:04:29, thành và càng lúc về sau nó sẽ càng giảm
0:04:26 - 0:04:31, xuống và cứ thêm một cái mô hình mới thì
0:04:29 - 0:04:34, cái sai số của mình nó sẽ càng lúc càng
0:04:31 - 0:04:37, giảm thì cái hình bên phải minh họa cái
0:04:34 - 0:04:40, ý tưởng của thuật toán boosting đầu tiên
0:04:37 - 0:04:43, đó là chúng ta sẽ có một cái tập dữ liệu
0:04:40 - 0:04:48, à gốc và tập dữ liệu này thì cũng tương
0:04:43 - 0:04:51, tự nó sẽ chia ra làm các cái data 1 2 3
0:04:48 - 0:04:53, cho đến data thứ N. Ta sẽ thực hiện cái
0:04:51 - 0:04:55, thao tác là chia ra và chúng ta sẽ tiến
0:04:53 - 0:04:59, hành huấn luyện trên cái model số 1
0:04:55 - 0:05:02, trước rồi sau đó Cái model số 1 này
0:04:59 - 0:05:03, chúng ta sẽ thực hiện trên cái data
0:05:02 - 0:05:07, chúng ta sẽ tiến hành đi predict trên
0:05:03 - 0:05:10, cái data số 2 và chúng ta sẽ ra được
0:05:07 - 0:05:12, cái sai số với cái sai số này thì chúng
0:05:10 - 0:05:14, ta sẽ tiếp tục khai thác cái sai số đó để
0:05:12 - 0:05:17, mà đi train trên cái model số 2 Model số
0:05:14 - 0:05:19, 2 sau khi đã được train xong sẽ đi
0:05:17 - 0:05:22, predict trên cái dữ liệu số 3 và chúng
0:05:19 - 0:05:27, ta sẽ ra được cái sai số và dựa trên cái
0:05:22 - 0:05:30, sai số của model số 3 chúng ta sẽ tiếp tục
0:05:27 - 0:05:33, đi train cho cái model số 3 rồi lại
0:05:30 - 0:05:35, tiếp tục cứ như vậy cho đến data thứ N
0:05:33 - 0:05:38, và chúng ta sẽ train cái model thứ N và
0:05:35 - 0:05:43, N cái model này sẽ được tổng hợp lại với
0:05:38 - 0:05:46, một cái kỹ thuật ensemble thì ensemble này
0:05:43 - 0:05:48, có thể là những kỹ thuật cơ bản như là
0:05:46 - 0:05:50, voting averaging hoặc bản thân cái
0:05:48 - 0:05:52, phương pháp ensemble này nó được xuất phát từ
0:05:50 - 0:05:55, cái thuật toán của mình tức là cái thuật
0:05:52 - 0:05:57, toán kết hợp giữa Model số 1 Model số 2
0:05:55 - 0:05:59, Model số 3 với những cái trọng số mà mô
0:05:57 - 0:06:01, hình của mình nó đã được học trong cái
0:05:59 - 0:06:04, quá trình boosting
0:06:01 - 0:06:06, thì để hiểu rõ hơn về kỹ thuật Boosting thì
0:06:04 - 0:06:08, chúng ta sẽ lấy một cái thuật toán đại
0:06:06 - 0:06:11, diện đó chính là thuật toán Gradient Boost
0:06:08 - 0:06:15, và Gradient Boost ý tưởng của nó đó
0:06:11 - 0:06:18, là nó sẽ xây dựng một cái chuỗi một cái
0:06:15 - 0:06:20, chuỗi các cái cây quyết định liên tiếp
0:06:18 - 0:06:23, với nhau thì nếu như bagging nó chỉ nói
0:06:20 - 0:06:27, chung chung là một cái mô hình thì trong
0:06:23 - 0:06:30, trường hợp này chúng ta sẽ sử dụng là
0:06:27 - 0:06:33, cây quyết định là một cái mô hình cụ thể
0:06:30 - 0:06:37, và kế thừa cái ý tưởng của bagging trong
0:06:33 - 0:06:39, cái thuật toán bagging thì nó sẽ có cái
0:06:37 - 0:06:42, random Forest và random Forest thì cái
0:06:39 - 0:06:45, model thành phần của mình là họ cũng sử
0:06:42 - 0:06:48, dụng cây quyết định decision tree thì
0:06:45 - 0:06:50, boosting nó cũng sẽ sử dụng cái mô hình
0:06:48 - 0:06:53, thành phần là decision tree, cây quyết
0:06:50 - 0:06:57, định và nhiệm vụ đó là cái cây sau sẽ cố
0:06:53 - 0:06:59, gắng làm giảm cái sai số dự đoán của các
0:06:57 - 0:07:02, cái cây trước đó ví dụ như chúng ta thấy
0:06:59 - 0:07:05, ở đây là có cái cột là residual tức là
0:07:02 - 0:07:07, cái sai số dự đoán nếu như với với cái
0:07:05 - 0:07:10, kết quả khởi tạo cái cây đầu tiên là
0:07:07 - 0:07:14, thật ra nó cũng không phải là cây đó là
0:07:10 - 0:07:16, cái giá trị trung bình của cái cột giá
0:07:14 - 0:07:19, trị dự đoán thì sai số nếu như chúng ta
0:07:16 - 0:07:22, sử dụng cái giá trị trung bình này như
0:07:19 - 0:07:25, là một cái giá trị dự đoán thì sai số
0:07:22 - 0:07:27, của mình sẽ là cái cái giá trị ở trên
0:07:25 - 0:07:30, cái cột thứ nhất nhưng nếu như chúng ta
0:07:27 - 0:07:33, kết hợp với lại cái cây thứ
0:07:30 - 0:07:37, hai và với một cái hệ số kết hợp thì cái
0:07:33 - 0:07:40, sai số của mình nó giảm từ 16.8 xuống
0:07:37 - 0:07:43, còn 15.1 từ 4.8 xuống còn 4.3 từ -15.2
0:07:40 - 0:07:48, xuống còn - 13.7 Và khi chúng ta kết hợp
0:07:43 - 0:07:51, thêm với cái cây thứ ba
0:07:48 - 0:07:53, thì cái cái sai số mình nó đã giảm từ 15
0:07:51 - 0:07:55, xuống còn 13 từ 4.3 xuống còn 3.9 từ -
0:07:53 - 0:07:59, 13.7 xuống còn -12.1. thì cái hình
0:07:55 - 0:08:01, này nó lấy từ cái nguồn video với cái
0:07:59 - 0:08:04, đường dẫn như sau và bên phải đó là một
0:08:01 - 0:08:07, cái hình hình ảnh minh họa khác sử dụng
0:08:04 - 0:08:10, animation đó thì cái đường màu xanh
0:08:07 - 0:08:12, chính là cái hàm đúng của mình còn cái
0:08:10 - 0:08:14, đường màu đỏ chính là
0:08:12 - 0:08:17, cái đường mà do thuật toán boosting tạo
0:08:14 - 0:08:19, lập ra thì chúng ta thấy đó là tại những
0:08:17 - 0:08:22, cái bước đầu tiên thì cái đường màu đỏ
0:08:19 - 0:08:27, nó sẽ rất là cách xa nhưng mà sau khi huấn
0:08:22 - 0:08:30, luyện xong thì cái đường màu đỏ nó đã xấp
0:08:27 - 0:08:33, xỉ với cái đường màu màu xanh đó thì đây
0:08:30 - 0:08:35, chính là cái hình ảnh à minh họa cho cái
0:08:33 - 0:08:38, thuật toán đó. V như chúng ta thấy ở đây
0:08:35 - 0:08:43, mới vòng lặp đầu tiên thì cái đường của
0:08:38 - 0:08:46, mình nó rất là thô và nó sẽ đi ra không
0:08:43 - 0:08:48, có giống lắm Nó sẽ không có mô phỏng giống lắm
0:08:46 - 0:08:51, cái đường mà xanh nhưng mà
0:08:48 - 0:08:54, càng chạy thì cái đường Zig zắc này sẽ
0:08:51 - 0:08:58, càng lúc nó càng mịn hơn và nó sẽ càng
0:08:54 - 0:09:00, tiến sát hơn đến cái đường màu xanh của
0:08:58 - 0:10:07, mình thì để có thể hiểu rõ hơn và trực
0:10:07 - 0:10:11, quan hơn cho cái thuật toán Gradient Boost thì
0:10:11 - 0:10:16, chúng ta sẽ cùng theo dõi cái video ở
0:10:14 - 0:10:19, YouTube như
0:10:16 - 0:10:22, sau chúng ta sẽ có một cái bảng dữ liệu ba
0:10:19 - 0:10:25, cái cột đầu tiên sẽ là ba cái cột đặc
0:10:22 - 0:10:28, trưng đầu vào và cái cột khối lượng cái cột
0:10:25 - 0:10:30, khối lượng cuối cùng chính là cái cột mà
0:10:28 - 0:10:32, mình cần phải dự đoán thì thuật toán
0:10:30 - 0:10:35, Gradient Boost sẽ chạy
0:10:32 - 0:10:37, lên cái cột này để giá trị của cái cột
0:10:35 - 0:10:40, khối lượng này thì bước đầu tiên đó là
0:10:37 - 0:10:41, chúng ta sẽ đoán cái giá trị trung bình
0:10:40 - 0:10:45, à chúng ta sẽ đoán bằng cách đó là lấy
0:10:41 - 0:10:48, một cái giá trị trung bình thì tất cả
0:10:45 - 0:10:51, những cái mẫu nào mà đưa vào chúng ta
0:10:48 - 0:10:54, cũng sẽ đưa ra cái Phán đoán là cái giá
0:10:51 - 0:10:58, trị mà khối lượng trung bình này đó là 71.2 và
0:10:54 - 0:11:00, đương nhiên là cái việc sử dụng giá trị
0:10:58 - 0:11:04, trung bình này nó sẽ dẫn đến là cái sai
0:11:00 - 0:11:08, số nó sẽ dẫn đến cái sai số
0:11:04 - 0:11:10, thì với cái sai số này á thì chúng ta sẽ
0:11:08 - 0:11:14, tiếp tục chỉnh sửa lại
0:11:10 - 0:11:17, à Ví dụ như ở đây chúng ta có cái mẫu dữ
0:11:14 - 0:11:19, liệu đầu tiên ha mẫu cái cái dòng đầu
0:11:17 - 0:11:23, tiên cái dòng dữ liệu đầu tiên khi chúng
0:11:19 - 0:11:25, ta lấy cái
0:11:23 - 0:11:31, Ờ giá trị Dự đoán mà sử dụng cái giá trị
0:11:25 - 0:11:34, trung bình là 71.2 này thì khi chúng ta
0:11:31 - 0:11:39, Trừ lấy cái giá trị thực là y là ở đây
0:11:34 - 0:11:42, là 88 trừ cho cái giá trị trung bình mà
0:11:39 - 0:11:45, mình dự đoán là 71.2 thì chúng ta sẽ ra
0:11:42 - 0:11:48, được cái sai số là 16.8
0:11:45 - 0:11:51, và cứ tương tự như vậy thì chúng ta sẽ
0:11:48 - 0:11:54, ra được
0:11:51 - 0:12:00, cái nguyên cái sai số này đó
0:11:54 - 0:12:02, là 4.8 76 trừ cho giá trị là 71.2 ấy
0:12:00 - 0:12:05, chúng ta sẽ ra là 4.8 rồi sai số là -15.2
0:12:02 - 0:12:07, vân vân và bây giờ chúng ta sẽ không còn
0:12:05 - 0:12:10, quan tâm chúng ta sẽ không còn quan tâm
0:12:07 - 0:12:12, đến
0:12:10 - 0:12:15, cái cái cột khối lượng ban đầu này nữa mà cái
0:12:12 - 0:12:18, cột mà chúng ta cần phải dự đoán tiếp
0:12:15 - 0:12:20, theo đó chính là cái cột sai số này và
0:12:18 - 0:12:22, với input feature nó vẫn là ba cái cột
0:12:20 - 0:12:27, đầu
0:12:22 - 0:12:28, tiên và sau khi xây dựng cái thuật toán
0:12:27 - 0:12:31, decision tree trên cái cột dữ liệu mới
0:12:28 - 0:12:34, này thì chúng ta sẽ ra được một cái cây
0:12:31 - 0:12:36, quyết định chúng ta sẽ ra một cái cây
0:12:34 - 0:12:40, quyết
0:12:36 - 0:12:44, định và chúng ta sẽ không sử dụng cái
0:12:40 - 0:12:47, cây quyết định này một cách trọn vẹn 100%
0:12:44 - 0:12:50, mà chúng ta sẽ phải sử dụng kết hợp có
0:12:47 - 0:12:50, tham
0:12:50 - 0:12:59, số đó nó gọi là hệ số learning rate bình
0:12:59 - 0:13:05, thường cái hệ số này là 1 nhưng mà cứ
0:13:05 - 0:13:12, mỗi một cái vòng lặp thì cái cái cái cái
0:13:12 - 0:13:19, cái mức độ mà chúng ta khai thác cái cây
0:13:19 - 0:13:25, mới nó chỉ là một cái hệ số dưới một thôi
0:13:25 - 0:13:29, tức là chúng ta sẽ à cộng với lại một
0:13:29 - 0:13:33, cái hệ số tương đối nhỏ để cho cái quá
0:13:31 - 0:13:37, trình này của mình nó sẽ cập nhật dần dần
0:13:33 - 0:13:40, và đương nhiên cái hệ số learning rate càng nhỏ thì cái số cây mà mình tạo
0:13:37 - 0:13:41, ra nó cũng sẽ càng nhiều cũng sẽ càng lớn
0:13:40 - 0:13:44, để khắc phục cái sai sót do cái quá
0:13:41 - 0:13:48, trình dự đoán thì trong trường hợp này chúng ta
0:13:44 - 0:13:52, sẽ sử dụng à Đó là cái trọng số là 0.1
0:13:48 - 0:13:55, như vậy thì với một cái mẫu dữ liệu là
0:13:52 - 0:13:58, đầu tiên là cái hàng đầu tiên thì cái
0:13:55 - 0:14:02, giá trị dự đoán của mình nó sẽ là
0:13:58 - 0:14:06, bằng 71.2 Tức là cái giá trị trung bình
0:14:02 - 0:14:11, đầu tiên ở cái bước đầu tiên cộng cho
0:14:06 - 0:14:14, cộng cho giả sử như với cái đặc trưng là
0:14:11 - 0:14:17, height, color và gender này nó đi theo cái
0:14:14 - 0:14:21, con đường này nó ra cái giá trị dự đoán
0:14:17 - 0:14:24, của cái sai số là 16.8 nhưng 16.8 này nó
0:14:21 - 0:14:28, phải nhân với hệ số learning rate là 0.1 Tức là
0:14:24 - 0:14:30, nó chỉ cộng với một cái tỉ lệ là 10%
0:14:28 - 0:14:32, cái thông tin từ cái cây quyết định này
0:14:30 - 0:14:36, ra thôi như vậy thì
0:14:32 - 0:14:39, 71.2 cộng cho 0.1 nhân cho 16.8 nó ra là
0:14:36 - 0:14:39, 71
0:14:39 - 0:14:46, 72.9 chúng ta sẽ lấy cái tính cái sai số
0:14:43 - 0:14:48, tiếp theo chúng ta sẽ tính cái sai số
0:14:46 - 0:14:48, tiếp
0:14:52 - 0:14:58, theo rồi cái sai số tiếp theo nó sẽ được
0:14:55 - 0:15:01, tính bằng cách đó là nó vẫn sẽ lấy cái
0:15:01 - 0:15:08, giá trị trọng khối lượng gốc ban đầu là
0:15:05 - 0:15:12, 88 trừ cho cái giá trị Dự đoán khi đã có
0:15:08 - 0:15:15, sự kết hợp của giá trị trung bình ở đây
0:15:12 - 0:15:19, với lại cái cây mà chúng ta đã tạo ra ở
0:15:15 - 0:15:23, cái lớp trước đó Cái biến đổi trước đó
0:15:19 - 0:15:26, thì 88 trừ cho cái giá trị Dự đoán mới
0:15:23 - 0:15:28, này thì nó sẽ ra cái sai số của mình là
0:15:26 - 0:15:31, 15.1 và chúng ta thực hiện tương tự như
0:15:28 - 0:15:31, vậy cho tất cả các cái dòng dữ liệu còn
0:15:31 - 0:15:31, lại
0:15:42 - 0:15:49, rồi và sau khi chúng ta đã thực hiện thì
0:15:46 - 0:15:51, chúng ta đã có một cái giá trị độ lỗi
0:15:49 - 0:15:53, mới nó khác so với lại cái độ lỗi ở cái
0:15:51 - 0:15:55, bước tính là chỉ có mỗi cái giá trị
0:15:53 - 0:15:57, trung bình thôi thì với cái cây mới này
0:15:55 - 0:16:00, chúng ta có một cái giá trị độ lỗi mới
0:15:57 - 0:16:03, và lặp lại À cái việc cái cái cái quá
0:16:00 - 0:16:06, trình thực hiện này chúng ta lại đi tiếp
0:16:03 - 0:16:07, tục huấn luyện à chúng ta sẽ tiếp tục đi
0:16:06 - 0:16:11, huấn
0:16:07 - 0:16:14, luyện cho cái residual mới này và chúng
0:16:11 - 0:16:16, ta tạo ra được một cái cây mới Cứ lập đi
0:16:14 - 0:16:19, lặp lại như vậy chúng ta sẽ tạo ra được
0:16:16 - 0:16:22, một cái cây mới Và cái cây mới này sẽ
0:16:19 - 0:16:25, được kết hợp nó sẽ được kết hợp với lại
0:16:22 - 0:16:28, hai cái cây trước đó đó là cái cây ở đây
0:16:25 - 0:16:31, và cái giá trị trung bình ở đây và cách
0:16:28 - 0:16:33, mà chúng ta kết hợp là chúng ta vẫn phải
0:16:31 - 0:16:36, phải nhân với một cái hệ số learning rate
0:16:33 - 0:16:41, là 0.1 tức là chúng ta chỉ lấy khoảng 10%
0:16:36 - 0:16:41, cái giá trị Dự đoán mà
0:16:42 - 0:16:52, thôi rồi và ở đây thì chúng ta sẽ có cái
0:16:48 - 0:16:54, ba cái residual tương ứng cho ba cái lần
0:16:52 - 0:16:57, mà chúng ta học cái mô hình của mình ở
0:16:54 - 0:17:01, cái lần đầu tiên với chỉ có duy nhất một
0:16:57 - 0:17:03, cái giá trị trung bình là 71.2 thì chúng
0:17:01 - 0:17:09, ta thấy là cái độ lỗi của mình nó là
0:17:03 - 0:17:12, 16.8 nhưng khi có cái sự kết hợp 0.1 tức
0:17:09 - 0:17:16, là 10% của cái cây thứ hai thì độ lỗi
0:17:12 - 0:17:20, của mình là nó rớt xuống còn 15.1 và khi
0:17:16 - 0:17:22, kết hợp với lại cái cây thứ ba thì cái
0:17:20 - 0:17:25, độ lỗi của mình nó sẽ giảm xuống Khoảng
0:17:22 - 0:17:28, 13.6 Và cứ như vậy chúng ta cứ tạo ra
0:17:25 - 0:17:30, thêm nhiều cái cây và khi tạo ra một cái
0:17:28 - 0:17:33, cây mới thì chúng ta lại tạo ra một cái
0:17:30 - 0:17:36, residual mới một cái sai số mới cái cột
0:17:33 - 0:17:39, sai số mới Và cái cây huấn luyện nó sẽ
0:17:36 - 0:17:42, được huấn luyện trên cái cột sai số mới
0:17:39 - 0:17:44, chứ không phải là dựa trên cái À cái cái
0:17:42 - 0:17:47, cái cột mà Weight Tức là cái cột khối
0:17:44 - 0:17:49, lượng ban đầu đó thì cứ như vậy thì thuật toán
0:17:47 - 0:17:52, của mình nó sẽ chạy thì đây chính
0:17:49 - 0:17:55, là cái ý tưởng của
0:17:52 - 0:18:01, ờ thuật toán Gradient
0:17:55 - 0:18:04, Boost và quay trở lại cái
0:18:01 - 0:18:07, kỹ thuật boosting thì bên cạnh thuật
0:18:04 - 0:18:09, toán Gradient Boost thì chúng ta sẽ có
0:18:07 - 0:18:13, rất nhiều những cái thuật toán boosting
0:18:09 - 0:18:16, nổi tiếng khác đó chính là adaboost rồi
0:18:13 - 0:18:20, XGBoost, LightGBM và
0:18:16 - 0:18:22, CatBoost và tất cả những cái thuật toán này
0:18:20 - 0:18:24, đều là những cái thuật toán mà rất là
0:18:22 - 0:18:28, nổi tiếng và đạt được những cái giải cao
0:18:24 - 0:18:30, trong cuộc thi Kaggle trong đó nổi tiếng nhất
0:18:28 - 0:18:33, đó chính là ba cái cái thuật toán này: L
0:18:30 - 0:18:35, GBM, LightGBM và CatBoost. Đây là ba cái thuật toán
0:18:33 - 0:18:37, mà được sử dụng rất là nhiều trong các
0:18:35 - 0:13:40, cái cuộc thi Kaggle và đạt được thứ
0:13:37 - 0:13:43, hạng rất là cao trong đó XGBoost là một
0:13:40 - 0:13:47, cái cải tiến của Gradient Boost. CatBoost cũng
0:13:43 - 0:13:49, là cải tiến. LightGBM thì nghe cái từ Light
0:13:47 - 0:13:51, thôi là chúng ta đã biết là mục tiêu của
0:13:49 - 0:13:54, nó là làm gì rồi đúng không? Tức
0:13:51 - 0:13:56, là chúng ta đang tăng cái tốc
0:13:54 - 0:13:59, độ liên quan đến cái yếu tố về mặt tốc
0:13:56 - 0:14:01, độ đó thì trong cái thuật toán Gradient
0:13:59 - 0:14:02, Boost chúng ta thấy là có cái hệ số gọi
0:14:01 - 0:14:05, là learning
0:14:02 - 0:14:08, rate thì nếu như cái hệ số learning rate
0:14:05 - 0:14:12, này mà càng nhỏ thì dẫn đến là cái số
0:14:08 - 0:14:14, lượng cây của mình sẽ càng tăng và như
0:14:12 - 0:14:16, vậy thì cái chi phí tính toán của mình
0:14:14 - 0:14:19, rồi khi chúng ta huấn luyện cũng như là
0:14:16 - 0:14:21, chi phí khi chúng ta inference tức là
0:14:19 - 0:14:23, khi chúng ta dự đoán nó cũng rất là cao
0:14:21 - 0:14:27, thì LightGBM nó sẽ giúp cho chúng ta giải
0:14:23 - 0:14:29, quyết vấn đề này. Như vậy, tóm lại, ensemble
0:14:27 - 0:14:33, learning Đó là một cái kỹ thuật quan
0:14:29 - 0:14:35, trọng để cho mình có thể giúp mô hình để mô
0:14:33 - 0:14:38, hình của mình nó có thể tổng quát hóa
0:14:35 - 0:14:40, được thì mô hình của mình là nó sẽ được
0:14:38 - 0:14:43, học trên rất nhiều những cái loại dữ
0:14:40 - 0:14:46, liệu khác nhau cũng như là khai thác
0:14:43 - 0:14:48, được những cái điểm mạnh của từng cái mô
0:14:46 - 0:14:50, hình thành phần và bổ trợ cho nhau để
0:14:48 - 0:14:52, giải quyết những cái điểm yếu của từng
0:14:50 - 0:14:55, cái mô hình đó thì mô hình của mình nó
0:14:52 - 0:14:57, sẽ nhiều cái mô hình yếu nó tạo ra thành
0:14:55 - 0:14:59, một cái mô hình mạnh hay nói cách khác
0:14:57 - 0:15:02, đó là mô hình của mình nó càng tổng quát
0:14:59 - 0:15:04, và trong số những cái kỹ thuật về
0:15:02 - 0:15:07, ensemble learning thì bagging và Boosting là
0:15:04 - 0:15:10, hai cái kỹ thuật cho cái tính hiệu quả
0:15:07 - 0:15:14, rất là cao và nó là những cái hai cái
0:15:10 - 0:15:18, hướng tiếp cận mà được các cái cuộc thi
0:15:14 - 0:15:20, trên Kaggle là họ sử dụng rất là nhiều và
0:15:18 - 0:15:23, lưu ý đó là trong cái quá trình mà sử
0:15:20 - 0:15:26, dụng thì tất cả cái mô hình này nó đều có
0:15:23 - 0:15:28, những cái siêu tham số và chúng ta sẽ
0:15:26 - 0:15:30, phải chọn các cái siêu tham số làm sao
0:15:28 - 0:15:32, cho nó phù hợp bằng cái phương pháp gọi
0:15:30 - 0:15:34, là phương pháp tinh chỉnh tham số là cái
0:15:32 - 0:15:37, này nó nằm trong cái bài gọi là
0:15:34 - 0:15:40, parameter tuning. À này là nằm trong cái bài là
0:15:37 - 0:15:40, parameter tuning.
0:15:42 - 0:15:50, đó là sẽ tìm xem cái bộ siêu tham
0:15:47 - 0:15:53, số nào mà tối ưu cho các cái thuật toán
0:15:50 - 0:15:53, của mình