0:00:00 - 0:00:05, Chủ đề, học sâu, machine learning, ImageNet.
0:00:05 - 0:00:10, Chủ đề, học sâu, machine learning, ImageNet.
0:00:30 - 0:00:32, cho bài toán hồi quy.
0:00:33 - 0:00:35, Quá trình sẽ gồm hai bước.
0:00:35 - 0:00:39, Đầu tiên, chúng ta sẽ chia không gian đặc trưng.
0:00:39 - 0:00:45, Không gian đặc trưng ở đây là tập hợp các giá trị có thể của
0:00:45 - 0:00:51, x1, x2, xp hay còn gọi là các cột đặc trưng
0:00:51 - 0:00:57, thành G vùng riêng biệt R1, R2 và Rg.
0:00:57 - 0:01:00, R, ở đây là viết tắt của region
0:01:02 - 0:01:07, Tiếp theo, với mỗi mẫu trong vùng R_g
0:01:07 - 0:01:14, chúng ta đưa ra một giá trị dự đoán cho tất cả các mẫu thuộc về vùng đó
0:01:14 - 0:01:22, chính là giá trị trung bình của các giá trị phản hồi cho các mẫu huấn luyện trong vùng R_g
0:01:22 - 0:01:26, Giá trị phản hồi ở đây thực ra là giá trị hồi quy các bạn.
0:01:34 - 0:01:40, Như vậy thì bây giờ chúng ta sẽ quan tâm quá trình phân tầng không gian đặc trưng
0:01:40 - 0:01:42, nó sẽ diễn ra như thế nào
0:01:42 - 0:01:49, và là thế nào chúng ta sẽ xây dựng các vùng R1, R2 và R_g.
0:01:49 - 0:01:55, Đầu tiên chúng ta sẽ quan tâm đặc điểm của các vùng này
0:01:55 - 0:02:08, Các vùng này sẽ được giới hạn trong các vùng có dạng hình chữ nhật và song song với các trục hay còn được gọi là các đặc trưng
0:02:08 - 0:02:20, sau khi chúng ta quan tâm tới đặc điểm của các vùng, bước tiếp theo chúng ta sẽ đi tìm mục tiêu của bài toán
0:02:20 - 0:02:27, đó là chúng ta sẽ cố gắng giảm thiểu giá trị Residual Sum Square
0:02:27 - 0:02:43, Giả sử chúng ta sẽ có G vùng và chúng ta sẽ đi duyệt từng vùng
0:02:43 - 0:02:52, và ứng với mỗi vùng, chúng ta sẽ đi duyệt từng mẫu trong vùng đó
0:02:52 - 0:03:01, và chúng ta sẽ đi tính độ lỗi của giá trị dự đoán
0:03:01 - 0:03:10, Y_i là giá trị thật của mẫu thứ i
0:03:10 - 0:03:18, Y_i dự đoán cho vùng G được định nghĩa như sau
0:03:18 - 0:03:27, Ở đây để dễ hình dung hơn thì giả sử chúng ta sẽ có 1 cái cây có 3 vùng
0:03:27 - 0:03:31, Chúng ta sẽ tưởng tượng sẽ có 1 cái cây như thế này
0:03:31 - 0:03:36, Chúng ta có 3 vùng, ở đây thì chỉ số G sẽ chạy
0:03:36 - 0:03:43, Chúng ta sẽ có vùng thứ nhất, vùng thứ 2 và vùng thứ 3
0:03:43 - 0:03:47, Trong này có 3 điểm dữ liệu
0:03:47 - 0:03:53, Y của chúng ta sẽ chạy trong 3 mẫu này
0:03:53 - 0:03:59, Ở đây chúng ta chỉ có 2 mẫu và ở đây chúng ta có 4 mẫu
0:03:59 - 0:04:12, G bằng 1, G bằng 2, và G bằng 3
0:04:12 - 0:04:20, như vậy thì chúng ta có thể lưu ý rằng
0:04:20 - 0:04:27, G của chúng ta sẽ tương đương với số nút lá của cái cây quyết định
0:04:27 - 0:04:32, trong trường hợp này là cho bài toán hồi quy
0:04:32 - 0:04:39, như vậy thì chúng ta sẽ có rất nhiều cách để chia
0:04:39 - 0:04:44, không gian đặc trưng ban đầu thành các vùng
0:04:44 - 0:04:49, về mặt lý thuyết thì chúng ta có thể tìm ra tất cả các tổ hợp các vùng
0:04:49 - 0:04:53, sau đó chúng ta sẽ chọn
0:04:53 - 0:05:00, Cái cách phân vùng mà sao cho chúng ta sẽ có độ lỗi thấp nhất.
0:05:00 - 0:05:05, Tuy nhiên, việc này là không thể.
0:05:05 - 0:05:17, Trong thực tế, để thay thế cho thuật toán vét cạn thì người ta sẽ sử dụng thuật toán tham lam để đi giải quyết bài toán phân vùng.
0:05:17 - 0:05:31, Như vậy, chúng ta có thể thấy rằng, việc tìm ra tất cả các tổ hợp, các vùng quyết định, hay nói cách khác là tìm ra tất cả các cây quyết định cho bài toán hồi quy là điều không thể.
0:05:32 - 0:05:44, Như vậy, chúng ta sẽ tìm một hướng giải quyết khác. Đơn giản hơn, chúng ta sẽ tìm ra một lời giải không phải là hoàn hảo, tuy nhiên nhưng mang lại kết quả tốt.
0:05:44 - 0:05:47, Đó là thuật toán tham lam.
0:05:47 - 0:05:51, Ý tưởng của thuật toán này là phân chia nhị phân đệ quy.
0:05:51 - 0:05:55, Chúng ta sẽ tiếp cận phân chia vùng từ trên xuống.
0:05:55 - 0:06:02, Cụ thể là, đầu tiên chúng ta sẽ bắt đầu từ đỉnh của cây quyết định.
0:06:02 - 0:06:06, Tất cả các điểm đều thuộc về một vùng duy nhất.
0:06:06 - 0:06:18, Có nghĩa là tất cả các điểm trong bộ dữ liệu được hồi quy với một giá trị duy nhất nếu như chúng ta dừng lại thuật toán tại đây.
0:06:18 - 0:06:32, Nếu không, tiếp tục chúng ta sẽ tìm biến để phân chia hay nói cách khác là chúng ta sẽ đi lặp tất cả các cột đặc trưng trong dữ liệu đầu vào.
0:06:32 - 0:06:48, Ứng với mỗi cột đặc trưng như vậy, chúng ta sẽ tìm ra một điểm phân chia x, hay còn được viết tắt là split point
0:06:48 - 0:06:59, Đây là một cái từ mà chúng ta sẽ thấy quen thuộc trên các trang hướng dẫn của các diễn đàn về khoa học máy tính
0:06:59 - 0:07:12, Và chúng ta sẽ đi tìm điểm x sao cho giảm thiểu ResidualSumSquare nhiều nhất so với vùng ban đầu
0:07:12 - 0:07:24, Để dễ hình dung hơn, giả sử ban đầu tập dữ liệu của chúng ta sẽ gồm trên 2 điểm
0:07:24 - 0:07:34, Và chúng ta sẽ chia thành 2 vùng dữ liệu
0:07:34 - 0:07:39, Giả sử đây chúng ta sẽ split theo hướng này
0:07:39 - 0:07:44, Thì chúng ta sẽ có một số điểm bên trái và một số điểm bên phải
0:07:44 - 0:07:54, Và độ giảm giá trị lỗi sẽ là nhiều nhất khi chúng ta chọn các split này
0:07:54 - 0:08:05, Tiếp theo, chúng ta sẽ tiếp tục lặp lại cho từng vùng mà chúng ta đã chia tại bước đầu tiên
0:08:05 - 0:08:13, Tương tự cách ở bước 2 cho đến khi chúng ta sẽ thỏa mãn tiêu chí dừng
0:08:13 - 0:08:24, Bởi vậy đây là một thuật toán lặp cho nên chúng ta sẽ phải định nghĩa ra một tiêu chí dừng
0:08:24 - 0:08:33, Một điểm đáng lưu ý ở đây là thuật toán tham lam sẽ không nhìn về phía trước để xem xét các bước tiếp theo
0:08:33 - 0:08:46, có nghĩa là khi mà chúng ta phân chia các vùng ở bước này thì chúng ta không còn quan tâm đến kết quả của bước trước nữa
0:08:46 - 0:08:52, chúng ta coi như đây là một bộ dữ liệu mới
0:08:52 - 0:09:02, Ở các slide trước, chúng ta có đề cập đến việc các vùng của chúng ta sẽ là các hộp hình chữ nhật và song song với các trục
0:09:02 - 0:09:13, Ở đây, chúng ta sẽ có hai cách chia, hay là hai cách phân các vùng, cách bên trái và cách bên phải
0:09:13 - 0:09:26, Ở đây chúng ta có thể thấy rằng các vùng bên trái không thể được thực hiện bởi thuật toán mà chúng ta đã đề ra
0:09:26 - 0:09:37, Ứng với thuật toán mà chúng ta đã mô tả trong các phần trước thì tại mỗi bước chúng ta sẽ phân chia dữ liệu thành 2 vùng
0:09:37 - 0:09:51, Như vậy, chúng ta có thể hình dung là, giả sử ban đầu chúng ta sẽ có không gian đặc trưng gốc gồm x2 và x1
0:09:51 - 0:10:01, Tại bước đầu tiên, chúng ta sẽ đi lặp tất cả các điểm trong đặc trưng x1
0:10:01 - 0:10:05, sau đó chúng ta sẽ chọn ra điểm split point
0:10:06 - 0:10:09, có độ lỗi giảm nhiều nhất
0:10:10 - 0:10:16, tương tự như vậy chúng ta sẽ đi lặp tất cả các điểm split point trong đặc trưng x2
0:10:16 - 0:10:23, và chúng ta chọn ra một điểm có độ giảm lỗi nhiều nhất
0:10:23 - 0:10:29, và chúng ta sẽ coi hai giá trị giảm lỗi trong x1 và x2
0:10:29 - 0:10:34, bên nào có độ giảm lỗi cao nhất thì chúng ta sẽ tiến hành split
0:10:34 - 0:10:41, như vậy giả sử đây chúng ta sẽ có giá trị giảm lỗi trong x1 là nhiều nhất
0:10:41 - 0:10:44, và chúng ta sẽ tiến hành phân chia vùng
0:10:44 - 0:10:49, chúng ta chọn một điểm split point trong x1
0:10:49 - 0:10:53, và chúng ta sẽ tiến hành phân vùng
0:10:53 - 0:11:03, như vậy chúng ta có thể thấy rằng ít nhất sẽ có một đường thẳng
0:11:03 - 0:11:08, mà nó vuông góc với một trục đặc trưng
0:11:08 - 0:11:15, ở đây thì chúng ta sẽ để ý rằng trong cách phân vùng bên trái
0:11:15 - 0:11:24, không có đường thẳng nào mà cắt toàn bộ một vùng
0:11:24 - 0:11:30, như trong ví dụ mà chúng ta vừa minh họa
0:11:30 - 0:11:36, chúng ta có thể thấy rằng ở cách phân vùng thứ 2
0:0:11:36 - 0:11:42, đầu tiên thì chúng ta sẽ kẻ đường này để phân vùng
0:11:42 - 0:11:47, Tiếp theo, chúng ta sẽ có vùng bên trái và vùng bên phải
0:11:47 - 0:11:57, thì ở đây chúng ta sẽ có thể chia theo cách này
0:11:57 - 0:12:04, chúng ta coi như đây là một bộ dữ liệu độc lập chỉ còn R2 và R1
0:12:04 - 0:12:11, Tiếp theo, chúng ta coi phần dữ liệu bên phải là phần dữ liệu độc lập
0:12:11 - 0:12:16, thì chúng ta có thể split thành 2 phần
0:12:17 - 0:12:22, và tiếp theo chúng ta có thể coi cái phần còn lại là một bộ dữ liệu độc lập
0:12:22 - 0:12:24, và chúng ta sẽ split
0:12:29 - 0:12:32, và đây là một cách phân vùng hợp lệ
0:12:32 - 0:12:46, Trong các slide trước, đặc biệt là thuật toán xây dựng cây quyết định hồi quy, chúng ta có đề cập đến khái niệm tiêu chí dừng.
0:12:46 - 0:12:51, Đây là một cái điểm quan trọng để giúp chúng ta dừng thuật toán.
0:12:51 - 0:13:04, Về mặt lý thuyết, chúng ta sẽ dừng lại khi mà chúng ta phân chia các vùng, khi mà trong vùng đó chỉ còn một điểm dữ liệu duy nhất.
0:13:04 - 0:13:15, Ứng với tình huống đó thì chúng ta sẽ tạo ra một cái cây quyết định rất là lớn và nó sẽ tốn về mặt chi phí tính toán.
0:13:15 - 0:13:20, Thực tế thì nó có thể không tổng quát tốt cho dữ liệu mới.
0:13:20 - 0:13:31, Tại vì chúng ta có thể hình dung rằng bản chất của bài toán cây quyết định thì tại mỗi nút lá,
0:13:31 - 0:13:39, việc giá trị dự đoán nó sẽ có nghĩa là tất cả các điểm dữ liệu thuộc về một vùng.
0:13:39 - 0:13:47, Nó sẽ được dự đoán bằng giá trị trung bình tất cả các mẫu dữ liệu thuộc về vùng đó.
0:13:47 - 0:13:52, Hay nói cách khác, thì chúng ta dự đoán giá trị của một mẫu dữ liệu mới
0:13:52 - 0:13:59, nó sẽ là giá trị trung bình của những mẫu dữ liệu mà có đặc trưng tương đồng với nó nhất.
0:14:01 - 0:14:11, Thì tiêu chí dừng ở đây, có thể là chúng ta sẽ giới hạn kích thước của cây bằng cách kiểm soát hiện tượng quá khớp.
0:14:11 - 0:14:19, Hiện nay, các tiêu chí phổ biến để định nghĩa tiêu chí dừng cho cây quyết định bao gồm
0:14:19 - 0:14:28, số lượng mẫu trong một nút lá nhỏ hơn một số nhất định thì chúng ta sẽ dừng
0:14:28 - 0:14:35, độ tinh khiết của nút lớn hơn một số nhất định
0:14:35 - 0:14:48, Thực ra chúng ta sẽ có khái niệm về độ tinh khiết trong phần xây dựng cây quyết định cho bài toán phân loại
0:0:14:48 - 0:14:54, Độ tinh khiết ở đây thì chúng ta nghĩ đơn giản nó tương tự với độ thuần khiết thôi
0:14:54 - 0:15:06, Giả sử chúng ta sẽ có một dữ liệu phân loại gồm 3 mẫu thuộc lớp O và 1 mẫu thuộc lớp X
0:15:06 - 0:15:09, Đây là bộ dữ liệu thứ nhất ví dụ
0:15:09 - 0:15:14, Giả sử chúng ta sẽ tiếp tục có một bộ dữ liệu thứ 2
0:15:14 - 0:15:22, D2 gồm 4 mẫu dữ liệu thuộc về lớp O
0:15:22 - 0:15:28, thì chúng ta có thể thấy là giả sử chúng ta coi O như là nước cất
0:15:33 - 0:15:37, và X ở đây là 1
0:15:38 - 0:15:40, hạt sạn đi
0:15:40 - 0:15:49, Chúng ta có thể nói rằng bộ dữ liệu điểm 1 này chưa thuần khiết nước hoàn toàn,
0:15:49 - 0:15:55, còn bộ dữ liệu thứ 2 này được coi là nước thuần khiết.
0:15:55 - 0:16:05, Có nghĩa là nó sẽ không chứa các tạp chất khác.
0:16:05 - 0:16:17, Và trong bài toán này, chúng ta sẽ nói rằng một bộ dữ liệu hay một vùng nó thuần khiết một lớp nào đó
0:16:17 - 0:16:29, Chẳng hạn như chúng ta sẽ có một bộ D3, nó gồm 3 mẫu X, thì chúng ta có thể nói nó thuần khiết về một lớp X
0:16:29 - 0:16:37, Ngoài ra thì tiêu chí dừng của chúng ta sẽ có thể định nghĩa bằng độ sâu của 1 cây
0:16:37 - 0:16:45, Nếu như 1 cây mà có độ sâu quá lớn thì nó có thể là một hiện tượng quá khớp
0:16:45 - 0:16:53, và khi mà trong tình huống tất cả các giá trị phản hồi
0:16:53 - 0:17:00, ở đây là chúng ta sẽ hiểu là giá trị hồi quy đều giống hệt nhau hả?
0:17:00 - 0:17:12, có nghĩa là một cái cây hồi quy như thế này mà tất cả giá trị mà dự đoán cho bài toán hồi quy á
0:17:12 - 0:17:13, ở đây nó giống nhau luôn
0:17:13 - 0:17:14, v1
0:17:15 - 0:17:15, v1
0:17:16 - 0:17:17, v1
0:17:19 - 0:17:22, thì ở đây chúng ta có thể hình dung là giống như là trong cái môn
0:17:22 - 0:17:23, nhập môn lập trình đó
0:17:24 - 0:17:26, việc chúng ta viết các cái lệnh if
0:17:28 - 0:17:30, cái lệnh điều kiện đó các bạn
0:17:30 - 0:17:32, nhưng mà trong cái
0:17:35 - 0:17:35, cây quyết định
0:17:37 - 0:17:39, trong cái hàng cuối cùng á
0:17:39 - 0:17:41, thì tất cả các cái công việc nó đều như nhau
0:17:41 - 0:17:44, thì thôi chúng ta khỏi viết lệnh if thì còn hơn
0:17:45 - 0:17:50, Và tương tự như vậy, trong tình huống này, tất cả các giá trị phản hồi đều giống như hệt nhau
0:17:50 - 0:17:56, thì thôi chúng ta sẽ chỉ cần tạo ra một cái cây mà nó chỉ có một nút thôi
0:17:57 - 0:18:03, Nói cách khác, thì chúng ta thà không phân chia các vùng còn hơn
0:18:03 - 0:18:09, Chỉ đơn thuần là chúng ta kết luận là bộ dữ liệu này chỉ có một giá trị phản hồi mà thôi
0:18:11 - 0:18:24, Hãy subscribe cho kênh Ghiền Mì Gõ để không bỏ lỡ những video hấp dẫn